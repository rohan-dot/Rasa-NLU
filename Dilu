# helpers already defined earlier
#   – SYSTEM_MSG  (string)
#   – chat_wrap(role, text)  ⇒  "<|start_header_id|>role<|end_header_id|>\ntext<|eot_id|>\n"

def build_sample(example, add_labels: bool):
    """
    Build a single training / eval sample.
      • input_ids  : system + user + (assistant if training) + <eos>
      • labels     : identical length; prefix masked with -100
    """
    # 1) compose chat transcript ---------------------------------------------
    user_block = (
        f"Instruction: Inject CWE ID {example['cwe_id']} in the clean code\n"
        f"Input:\n{example['func_after']}\n"
        f"Output:"                               # assistant must continue here
    )
    assistant_block = example["func_before"] if add_labels else ""

    chat = (
        "<|begin_of_text|>\n"
        + chat_wrap("system",   SYSTEM_MSG)
        + chat_wrap("user",     user_block)
        + chat_wrap("assistant", assistant_block)
    )

    # 2) tokenize -------------------------------------------------------------
    tokens = tokenizer(
        chat, truncation=True, max_length=4096, add_special_tokens=False
    )["input_ids"]

    if tokens[-1] != tokenizer.eos_token_id:
        tokens.append(tokenizer.eos_token_id)

    # 3) build labels w/ safe masking ----------------------------------------
    if add_labels:
        labels = tokens.copy()  # same length
        # mask everything **before** assistant answer
        prefix = (
            "<|begin_of_text|>\n"
            + chat_wrap("system", SYSTEM_MSG)
            + chat_wrap("user",   user_block)
        )
        prefix_len = len(
            tokenizer(prefix, add_special_tokens=False)["input_ids"]
        )
        labels[:prefix_len] = [-100] * prefix_len
    else:
        labels = [-100] * len(tokens)

    return {"input_ids": tokens, "labels": labels}






xxxx

## BEND Counter‑Narrative Prompt Templates (No‑Emoji Version)

Below are four ready‑to‑paste templates—**EXPLAIN, ENHANCE, DISMISS, EXCITE**—refined to meet Dr. Kathleen Carley’s BEND objectives *without* using emojis or decorative symbols.

---

### 1️⃣ EXPLAIN (counter‑narrative)
```
<system>
You are a specialist in “EXPLAIN” counter‑narratives (BEND framework).
Mission: transform hateful tweets into calm, crystal‑clear replies that
• correct misinformation with one or two verified facts,
• add helpful context, and
• leave readers feeling smarter, not scolded.

STYLE
• Warm, human cadence—mix short and medium sentences.
• No emojis or decorative symbols.
• At most one lightweight hashtag if it adds clear value; otherwise none.

STRICT RULES
1. Do NOT repeat slurs or extremist slogans.
2. Keep final reply ≤ 280 characters.
3. Cite only facts supplied in CONTEXT; invent nothing.
4. Use plain, friendly language—sound human, not robotic.
5. Reveal ONLY the final tweet.
</system>

<user>
### INPUT
HATEFUL_TWEET:
{paste_hateful_tweet_here}

RELEVANT_CONTEXT (optional):
{short facts, stats, reputable source snippets}

### TASK (think step‑by‑step, hide chain‑of‑thought)
1  Spot the core false claim or stereotype.
2  Pick the strongest 1–2 context facts that clarify or correct it.
3  Craft a concise reply that states the correct fact(s) and offers brief context ("Actually…", "Fact: …").
4  Ensure ≤ 280 chars.
5  OUTPUT ONLY the tweet.

### OUTPUT FORMAT
COUNTER_NARRATIVE:
```{reply}```
</user>
```

---

### 2️⃣ ENHANCE (counter‑narrative)
```
<system>
You are a specialist in “ENHANCE” counter‑narratives (BEND framework).
Mission: reply to hateful tweets with messages that
• amplify solidarity and shared achievements,
• motivate people to keep supporting the targeted group/cause, and
• spotlight hopeful momentum.

STYLE
• Warm, human cadence; vivid but natural language.
• No emojis or decorative symbols.
• At most one lightweight hashtag if it adds clear value; otherwise none.

STRICT RULES
1. Never echo hateful rhetoric.
2. ≤ 280 characters.
3. Ground claims in CONTEXT if provided; invent nothing.
4. Encourage continued positive action ("Let’s…", "Keep building…").
5. Show ONLY the final tweet.
</system>

<user>
### INPUT
HATEFUL_TWEET:
{paste_hateful_tweet_here}

RELEVANT_CONTEXT (optional):
{success stories, statistics, quotes, upcoming events}

### TASK (think step‑by‑step, hide chain‑of‑thought)
1  Identify what the hate tries to tear down.
2  Choose 1–2 context items that showcase growth, success, or unity.
3  Write an energizing reply that highlights these wins and invites readers to join or continue the progress.
4  Ensure ≤ 280 chars.
5  OUTPUT ONLY the tweet.

### OUTPUT FORMAT
COUNTER_NARRATIVE:
```{reply}```
</user>
```

---

### 3️⃣ DISMISS (counter‑narrative)
```
<system>
You are a specialist in “DISMISS” counter‑narratives (BEND framework).
Mission: neutralize hateful tweets by
• down‑ranking their importance,
• pivoting attention to constructive or uplifting perspectives, and
• sapping the emotional charge from the hate.

STYLE
• Calm confidence; polite tone.
• No emojis or decorative symbols.
• No hashtags unless strictly necessary.

STRICT RULES
1. Never repeat hateful slurs.
2. ≤ 280 characters.
3. Stay civil; avoid sarcasm that could inflame.
4. If CONTEXT is provided, weave in a brief uplifting fact or broader perspective.
5. Show ONLY the final tweet.
</system>

<user>
### INPUT
HATEFUL_TWEET:
{paste_hateful_tweet_here}

RELEVANT_CONTEXT (optional):
{positive achievements, alternate topics, inspirational facts}

### TASK (think step‑by‑step, hide chain‑of‑thought)
1  Spot the hateful claim(s).
2  Select a positive fact or broader perspective that makes the hate feel small.
3  Compose a brief reply that politely dismisses the negativity and redirects toward something uplifting or forward‑looking.
4  Ensure ≤ 280 chars.
5  OUTPUT ONLY the tweet.

### OUTPUT FORMAT
COUNTER_NARRATIVE:
```{reply}```
</user>
```

---

### 4️⃣ EXCITE (counter‑narrative)
```
<system>
You are a specialist in “EXCITE” counter‑narratives (BEND framework).
Mission: transform hateful tweets into uplifting replies that
• radiate positivity and hope,
• celebrate shared humanity, and
• inspire constructive action.

STYLE
• Lively yet natural wording; no emojis.
• One concise hashtag allowed if it reinforces unity; otherwise none.

STRICT RULES
1. Never repeat or elaborate hateful slurs.
2. Keep final reply ≤ 280 characters.
3. Ground statements in CONTEXT if provided; invent nothing.
4. Reveal ONLY the final tweet.
</system>

<user>
### INPUT
HATEFUL_TWEET:
{paste_hateful_tweet_here}

RELEVANT_CONTEXT (optional):
{facts, source snippets, personal stories, statistics}

### TASK (think step‑by‑step, hide chain‑of‑thought)
1  Identify the hateful claim(s) or stereotypes.
2  Pick 1–2 context facts that challenge or reframe the hate.
3  Draft a warm, upbeat, persuasive counter‑narrative that evokes hope and invites positive action.
4  Ensure ≤ 280 chars.
5  OUTPUT ONLY the tweet.

### OUTPUT FORMAT
COUNTER_NARRATIVE:
```{reply}```
</user>
```

---

**Usage:**
1. Choose the manoeuvre that matches your strategy.
2. Paste the hateful tweet and optional context into the template.
3. Run it through your LLM fine‑tuned model.
4. The model returns a single, ready‑to‑post counter‑tweet aligned with Carley’s BEND framework.


---

## Suggested Language Cues by Maneuver

### EXPLAIN
- Actually, the data show...
- Fact: ...
- For context, ...
- Let's set the record straight: ...
- Evidence says ...

### ENHANCE
- Keep the momentum going—...
- Let's build on these wins together.
- Great strides already made; let's go further.
- Together, we'll keep shaping progress.
- Your support is making a real difference.

### DISMISS
- Not worth our energy—let's focus on...
- Bigger picture: ...
- Let's turn our attention to what matters.
- Plenty to celebrate instead of dwelling on that.
- There's more constructive ground to cover.

### EXCITE
- Imagine what's next when we unite!
- Hope is everywhere—let's amplify it.
- Together, we rise and thrive.
- The future's bright—let's seize it!
- Let's write the next chapter of progress.












xxxxxx


def build_sample(example, add_labels: bool):
    # …  (same code that builds `chat` and gets `tokens`)
    tokens = tokenizer(chat,
                       truncation=True,
                       max_length=4096,
                       add_special_tokens=False)["input_ids"]

    if tokens[-1] != tokenizer.eos_token_id:
        tokens.append(tokenizer.eos_token_id)

    if add_labels:
        # length of everything before the assistant’s answer
        pre_len = len(
            tokenizer(
                "<|begin_of_text|>\n"
                + chat_wrap("system", SYSTEM_MSG)
                + chat_wrap("user",
                            f"Instruction: Inject CWE ID {example['cwe_id']} in the clean code\n"
                            f"Input:\n{example['func_after']}\nOutput:")
            , add_special_tokens=False)["input_ids"]
        )

        # build labels ***the safe way***: copy then mask
        labels = tokens.copy()              # same length by construction
        labels[:pre_len] = [-100] * pre_len # ignore system+user part
    else:
        labels = [-100] * len(tokens)

    return {"input_ids": tokens, "labels": labels}



# llama_cwe_train.py
# Fine‑tune LLaMA‑3‑70B‑Instruct with QLoRA on CVEFixes1 (CWE‑119 + CWE‑125)

# ---------- 1. pip installs (skip if environment already prepared) ----------
# !pip install -q transformers==4.40.1 peft==0.10.0 bitsandbytes==0.43.0 \
#               datasets==2.19.0 accelerate==0.28.0

import torch, random, os
from datasets import load_dataset
from transformers import (AutoTokenizer, AutoModelForCausalLM,
                          BitsAndBytesConfig, TrainingArguments, Trainer)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

# ---------- 2. Model / tokenizer ----------
BASE_MODEL = "meta-llama/Llama-3-70B-Instruct"    # HF repo (needs access)
BNB_CFG = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16
)

tokenizer = AutoTokenizer.from_pretrained(
    BASE_MODEL, use_fast=False, trust_remote_code=True
)
tokenizer.pad_token = tokenizer.eos_token  # LLaMA has no dedicated PAD
tokenizer.padding_side = "right"           # right‑pad for training

model = AutoModelForCausalLM.from_pretrained(
    BASE_MODEL, quantization_config=BNB_CFG,
    device_map="auto", trust_remote_code=True
)
model = prepare_model_for_kbit_training(model)

lora_cfg = LoraConfig(
    r=16, lora_alpha=32, lora_dropout=0.05, bias="none",
    target_modules=["q_proj", "k_proj", "v_proj",
                    "o_proj", "gate_proj", "up_proj", "down_proj"],
    task_type="CAUSAL_LM"
)
model = get_peft_model(model, lora_cfg)
model.gradient_checkpointing_enable()
model.config.use_cache = False

# ---------- 3. Data ------------------------------------------------------------------
ds = load_dataset("euisuh15/cveFixes1", split="c")   # C‑code slice
TARGET_CWES = {"119", "125"}
ds = ds.filter(lambda x: str(x["cwe_id"]) in TARGET_CWES)
print("Samples kept:", len(ds))

# 80 / 20 split (adjust as desired)
ds = ds.shuffle(seed=42)
split = ds.train_test_split(test_size=0.2, seed=42)
train_ds, val_ds = split["train"], split["test"]

# ---------- 4. Prompt template helpers -----------------------------------------------
def chat_wrap(role, text):
    return f"<|start_header_id|>{role}<|end_header_id|>\n{text}<|eot_id|>\n"

SYSTEM_MSG = (
    "You are a helpful assistant trained to inject C‑language "
    "vulnerabilities based on CWE IDs. When asked, you output ONLY the "
    "modified (vulnerable) C code."
)

def build_sample(example, add_labels: bool):
    """Return dict with input_ids & labels (labels masked before assistant)."""
    prompt_user = (
        f"Instruction: Inject CWE ID {example['cwe_id']} in the clean code\n"
        f"Input:\n{example['func_after']}\n"
        f"Output:"
    )
    assistant_answer = example["func_before"] if add_labels else ""
    
    chat = (
        "<|begin_of_text|>\n"
        + chat_wrap("system", SYSTEM_MSG)
        + chat_wrap("user", prompt_user)
        + chat_wrap("assistant", assistant_answer)
    )   # note: trailing <|end_of_text|> not required – eos token covers it
    
    tokens = tokenizer(
        chat, truncation=True, max_length=4096, add_special_tokens=False
    )["input_ids"]
    
    if tokens[-1] != tokenizer.eos_token_id:
        tokens.append(tokenizer.eos_token_id)
    
    # label masking: everything BEFORE assistant_answer is -100
    if add_labels:
        # find index where assistant content starts
        split_idx = tokenizer(
            "<|begin_of_text|>\n"
            + chat_wrap("system", SYSTEM_MSG)
            + chat_wrap("user", prompt_user),
            add_special_tokens=False
        )["input_ids"]
        prv_len = len(split_idx)
        labels = [-100]*prv_len + tokens[prv_len:]
    else:
        labels = [-100]*len(tokens)   # val set: will be replaced during Trainer eval
    
    return {"input_ids": tokens, "labels": labels}

train_ds = train_ds.map(lambda ex: build_sample(ex, True),
                        remove_columns=train_ds.column_names,
                        num_proc=4)
val_ds   = val_ds.map(lambda ex: build_sample(ex, True),
                      remove_columns=val_ds.column_names,
                      num_proc=4)

# ---------- 5. Data collator (pads BOTH inputs & labels) -----------------------------
def collator(batch):
    max_len = max(len(x["input_ids"]) for x in batch)
    for x in batch:
        pad_amt = max_len - len(x["input_ids"])
        x["input_ids"].extend([tokenizer.pad_token_id]*pad_amt)
        x["labels"].extend([-100]*pad_amt)
    return {
        "input_ids": torch.tensor([x["input_ids"] for x in batch], dtype=torch.long),
        "labels": torch.tensor([x["labels"] for x in batch], dtype=torch.long),
        "attention_mask": torch.tensor(
            [[1 if id!=tokenizer.pad_token_id else 0 for id in x["input_ids"]]
             for x in batch], dtype=torch.long)
    }

# ---------- 6. Training configuration ------------------------------------------------
args = TrainingArguments(
    output_dir="llama3-qlora-cwe119-125",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,      # effective batch 16
    num_train_epochs=3,
    learning_rate=3e-4,
    warmup_steps=100,
    bf16=True,
    logging_steps=10,
    save_strategy="epoch",
    evaluation_strategy="no",            # turn to "epoch" if you want eval
    label_names=["labels"],              # required with PEFT
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    # eval_dataset=val_ds,               # uncomment if you enable eval
    data_collator=collator,
)

trainer.train()

# ---------- 7. Save LoRA adapter -----------------------------------------------------
trainer.model.save_pretrained("llama3-qlora-cwe119-125")
tokenizer.save_pretrained("llama3-qlora-cwe119-125")
print("Training done. Adapter saved to ./llama3-qlora-cwe119-125")







xxxxxx


def format_prompt(cwe_id, clean_code, vulnerable_code=None):
    system_msg = "You are a helpful assistant trained to inject vulnerabilities based on CWE IDs."
    user_msg = f"Instruction: Inject CWE ID {cwe_id} in the clean code\nInput:\n{clean_code}\nOutput:"
    assistant_msg = vulnerable_code or ""

    prompt = (
        "<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n"
        f"{system_msg}<|eot_id|>\n"
        "<|start_header_id|>user<|end_header_id|>\n"
        f"{user_msg}<|eot_id|>\n"
        "<|start_header_id|>assistant<|end_header_id|>\n"
        f"{assistant_msg}<|eot_id|>"
    )
    return prompt







xxx
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig, TrainingArguments, Trainer
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model, TaskType
from datasets import load_dataset
import torch
import random

# Load dataset and filter to only CWE-125
full_dataset = load_dataset("euisuh15/cveFixes1", split="c")
dataset = full_dataset.filter(lambda x: x["cwe_id"] == "125")

# Shuffle and sample if needed (optional)
random.seed(42)
dataset = dataset.shuffle().select(range(min(len(dataset), 2000)))

# Split train/eval
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_data = dataset["train"]
eval_data = dataset["test"]

# Prompt format
prompt_template = """Instruction: Inject CWE ID {cwe_id} in the clean code
Input:
{clean}
Output:
"""

def format_prompt(cwe_id, clean_code):
    return prompt_template.format(cwe_id=cwe_id, clean=clean_code)

def preprocess(example):
    prompt = format_prompt(example["cwe_id"], example["func_after"])
    target = example["func_before"]
    full = prompt + target

    tokenized = tokenizer(full, truncation=True, max_length=2048, padding=False, add_special_tokens=False)
    input_ids = tokenized["input_ids"]
    if input_ids[-1] != tokenizer.eos_token_id:
        input_ids.append(tokenizer.eos_token_id)

    prompt_len = len(tokenizer(prompt, add_special_tokens=False)["input_ids"])
    labels = [-100] * prompt_len + input_ids[prompt_len:]
    labels = labels[:len(input_ids)]
    input_ids = input_ids[:len(labels)]
    return {"input_ids": input_ids, "labels": labels}

train_dataset = train_data.map(preprocess, remove_columns=train_data.column_names)
eval_dataset = eval_data.map(preprocess, remove_columns=eval_data.column_names)

# Tokenizer & Model
model_name = "meta-llama/Llama-3.3-70B-Instruct"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
model = prepare_model_for_kbit_training(model)

# LoRA config
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.05,
    bias="none",
    task_type=TaskType.CAUSAL_LM
)
model = get_peft_model(model, lora_config)

# Data collator
from transformers import DataCollatorForSeq2Seq
collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer,
    model=model,
    label_pad_token_id=-100,
    pad_to_multiple_of=8
)

# Training args (eval optional)
training_args = TrainingArguments(
    output_dir="llama3-qlora-cwe125",
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=3e-4,
    warmup_steps=100,
    bf16=True,
    logging_steps=10,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    report_to="none",
    label_names=["labels"],
    ddp_find_unused_parameters=False
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,  # comment this line if you want to skip evaluation
    data_collator=collator,
)

trainer.train()

# Save adapter
model.save_pretrained("llama3-qlora-cwe125")
tokenizer.save_pretrained("llama3-qlora-cwe125")









xxxxxx


.from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import PeftModel
import torch

# Load 70B base + QLoRA adapter
base_model = "meta-llama/Llama-3.3-70B-Instruct"  # must be the same as you trained on
adapter_path = "output-llama3.3-qlora-cve"        # your trained adapter path

# BitsAndBytes config for 4-bit inference
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16
)

# Load tokenizer and base model
tokenizer = AutoTokenizer.from_pretrained(base_model, use_fast=False, trust_remote_code=True)
tokenizer.pad_token = tokenizer.eos_token  # for left-padding during generation
tokenizer.padding_side = "left"

model = AutoModelForCausalLM.from_pretrained(
    base_model,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Load QLoRA adapter on top of base model
model = PeftModel.from_pretrained(model, adapter_path)
model.eval()



# Example clean function and CWE
cwe_id = "119"
clean_code = """
#include <stdio.h>
#include <string.h>

void handle_input(char *input) {
    char buffer[50];
    if (strlen(input) < sizeof(buffer)) {
        strcpy(buffer, input);
        printf("Received: %s\\n", buffer);
    } else {
        printf("Input too long.\\n");
    }
}
"""

# Match training prompt format
prompt = f"Instruction: Inject CWE ID {cwe_id} in the clean code\nInput:\n{clean_code}\nOutput:\n"

# Tokenize
inputs = tokenizer(prompt, return_tensors="pt", padding=True, truncation=True, max_length=2048)
inputs = {k: v.to(model.device) for k, v in inputs.items()}

# Generate vulnerable code
with torch.no_grad():
    output_ids = model.generate(
        **inputs,
        max_new_tokens=512,
        temperature=0.7,
        do_sample=True,           # or False for greedy
        top_k=50,
        top_p=0.95,
        num_beams=1,
        eos_token_id=tokenizer.eos_token_id
    )

# Decode only the new generation (after prompt)
generated_text = tokenizer.decode(output_ids[0][inputs["input_ids"].shape[1]:], skip_special_tokens=True)

print("=== Vulnerable Code Injected ===\n")
print(generated_text)






xxxxxx



def preprocess_eval(example):
    prompt = format_prompt(example["cwe_id"], example["func_after"])
    vuln_code = example["func_before"]

    # Tokenize prompt (input only)
    prompt_ids = tokenizer(prompt, truncation=True, max_length=2048, padding=False, add_special_tokens=False)["input_ids"]

    # Tokenize vulnerable code (as label)
    output_ids = tokenizer(vuln_code, truncation=True, max_length=2048, padding=False, add_special_tokens=False)["input_ids"]
    if output_ids[-1] != tokenizer.eos_token_id:
        output_ids.append(tokenizer.eos_token_id)

    # Pad output_ids to match prompt_ids if needed (not strictly required here, but safe)
    max_len = max(len(prompt_ids), len(output_ids))
    prompt_ids += [tokenizer.pad_token_id] * (max_len - len(prompt_ids))
    output_ids += [-100] * (max_len - len(output_ids))  # -100 for loss masking

    return {"input_ids": prompt_ids, "labels": output_ids}








def preprocess_train(example):
    prompt = format_prompt(example["cwe_id"], example["func_after"])
    vuln_code = example["func_before"]
    
    full_text = prompt + vuln_code
    tokenized = tokenizer(full_text, truncation=True, max_length=2048, padding=False, add_special_tokens=False)
    
    input_ids = tokenized["input_ids"]
    if input_ids[-1] != tokenizer.eos_token_id:
        input_ids.append(tokenizer.eos_token_id)
    
    # Compute prompt length
    prompt_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
    prompt_len = len(prompt_ids)
    
    # Make sure labels are same length as input_ids
    labels = [-100] * prompt_len + input_ids[prompt_len:]
    labels = labels[:len(input_ids)]  # clip in case label is longer
    input_ids = input_ids[:len(labels)]  # clip input to label length

    return {"input_ids": input_ids, "labels": labels}





xxx

!pip install transformers datasets accelerate peft bitsandbytes  # Install required packages if not already installed

from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model
import torch

# Model and tokenizer identifiers
model_name = "meta-llama/Llama-3.3-70B-Instruct"  # ensure you have access to this model

# 4-bit quantization configuration for QLoRA
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=torch.bfloat16  # use BF16 for faster computation on A100
)

# Load tokenizer and model in 4-bit mode
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",              # automatically allocate model layers to GPU(s)
    trust_remote_code=True
)

# Prepare model for training (necessary for 8-bit/4-bit fine-tuning)
model = prepare_model_for_kbit_training(model)

# Set pad token to eos to avoid warnings, and set padding strategy
tokenizer.pad_token = tokenizer.eos_token  # use EOS token as PAD
tokenizer.padding_side = "right"           # pad on the right for training

# Enable gradient checkpointing for memory efficiency
model.gradient_checkpointing_enable()
model.config.use_cache = False  # must disable cache for gradient checkpointing

from peft import TaskType

# Configure LoRA/QLoRA parameters
lora_config = LoraConfig(
    r=16,             # LoRA rank (increase to 32 if memory allows for potentially better results)
    lora_alpha=32,    # LoRA scaling (usually 2 * r)
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],  # target key linear layers&#8203;:contentReference[oaicite:7]{index=7}
    lora_dropout=0.05,  # dropout for LoRA layers (0.0-0.1). Using a small value to regularize.
    bias="none",
    task_type=TaskType.CAUSAL_LM
)

# Wrap the base model with LoRA adapters
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # Log the number of trainable parameters (LoRA) vs total



from datasets import load_dataset

# Load the CVEFixes1 dataset, C code split
dataset = load_dataset("euisuh15/cveFixes1", split="c")
# Split into training and validation sets (90/10 split)
dataset = dataset.train_test_split(test_size=0.1, seed=42)
train_data = dataset["train"]
eval_data  = dataset["test"]

# Define the prompt template format
def format_prompt(cwe_id, clean_code):
    """Format the instruction, input, and output prompt without the vulnerable code."""
    return f"Instruction: Inject CWE ID {cwe_id} in the clean code\nInput:\n{clean_code}\nOutput:\n"

# Preprocessing function for training set: combine prompt + output, mask prompt tokens in labels
def preprocess_train(example):
    prompt = format_prompt(example["cwe_id"], example["func_after"])
    vuln_code = example["func_before"]
    # Combine prompt and vulnerable code for full input sequence
    full_text = prompt + vuln_code
    # Tokenize the combined sequence
    tokenized = tokenizer(full_text, truncation=True, max_length=2048, add_special_tokens=False)
    input_ids = tokenized["input_ids"]
    # Append EOS token id at end of sequence if not already present
    if input_ids and input_ids[-1] != tokenizer.eos_token_id:
        input_ids.append(tokenizer.eos_token_id)
    # Create labels: prompt part as -100, output part as actual tokens (including EOS)
    # Determine boundary: prompt length (tokens for "Instruction...Output:\n")
    prompt_ids = tokenizer(prompt, add_special_tokens=False)["input_ids"]
    prompt_len = len(prompt_ids)
    labels = [-100] * prompt_len + input_ids[prompt_len:]
    return {"input_ids": input_ids, "labels": labels}

# Preprocessing function for validation set: keep prompt and output separate
def preprocess_eval(example):
    prompt = format_prompt(example["cwe_id"], example["func_after"])
    vuln_code = example["func_before"]
    # Tokenize prompt alone
    prompt_ids = tokenizer(prompt, truncation=True, max_length=2048, add_special_tokens=False)["input_ids"]
    # Tokenize output (vulnerable code) alone
    output_ids = tokenizer(vuln_code, truncation=True, max_length=2048, add_special_tokens=False)["input_ids"]
    # Append EOS to output_ids
    if output_ids and output_ids[-1] != tokenizer.eos_token_id:
        output_ids.append(tokenizer.eos_token_id)
    return {"input_ids": prompt_ids, "labels": output_ids}

# Apply preprocessing
train_dataset = train_data.map(preprocess_train, remove_columns=train_data.column_names)
eval_dataset  = eval_data.map(preprocess_eval, remove_columns=eval_data.column_names)

from transformers import DataCollatorForSeq2Seq

# Create a data collator that pads inputs and labels, using -100 for padded labels
data_collator = DataCollatorForSeq2Seq(
    tokenizer=tokenizer, 
    model=model, 
    label_pad_token_id=-100, 
    pad_to_multiple_of=8  # pad to multiple of 8 for efficiency (optional)
)

import numpy as np
from transformers import TrainingArguments, Trainer
try:
    import evaluate
    bleu_metric = evaluate.load("bleu")
except ImportError:
    bleu_metric = None

def compute_metrics(eval_pred):
    """Compute Exact Match and BLEU score for generated code vs reference."""
    predictions, labels = eval_pred
    # Decode predictions and references
    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)
    # Replace -100 in labels as padding, then decode
    decoded_labels = []
    for label_seq in labels:
        # Replace -100 with pad_token_id for decoding
        label_seq = [tokenizer.pad_token_id if token == -100 else token for token in label_seq]
        decoded_labels.append(tokenizer.decode(label_seq, skip_special_tokens=True))
    # Compute Exact Match
    exact_matches = [1 if pred.strip() == ref.strip() else 0 for pred, ref in zip(decoded_preds, decoded_labels)]
    em_rate = np.mean(exact_matches)
    # Compute BLEU (if evaluate library is available)
    if bleu_metric is not None:
        bleu_score = bleu_metric.compute(
            predictions=[pred.split() for pred in decoded_preds],
            references=[[ref.split()] for ref in decoded_labels]
        )["bleu"]
    else:
        bleu_score = None
    metrics = {"exact_match": em_rate}
    if bleu_score is not None:
        metrics["bleu"] = bleu_score
    return metrics

# Define training arguments
training_args = TrainingArguments(
    output_dir="output-llama3.3-qlora-cve",   # output directory for checkpoints and artifacts
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,
    num_train_epochs=3,
    learning_rate=3e-4,
    warmup_steps=100,
    bf16=True,                      # use bfloat16 precision if available
    optim="adamw_bnb_8bit",         # 8-bit Adam optimizer for efficiency
    logging_steps=10,
    evaluation_strategy="steps",
    eval_steps=100,
    save_strategy="steps",
    save_steps=100,
    save_total_limit=2,
    report_to=["none"],             # or ["wandb"] to use Weights & Biases
    ddp_find_unused_parameters=False  # for multi-GPU, avoids checking unused params (all LoRA params are used)
)

# Initialize the Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=None,
    predict_with_generate=True,         # enable text generation for evaluation
    generation_max_length=512,         # max tokens to generate for output
    # generation_kwargs={"num_beams": 1}  # you can add more generate params if desired (e.g., beam search)
)

# Start training
trainer.train()

# Save the trained LoRA adapter (this saves only the LoRA weights, not the full 70B model)
trainer.model.save_pretrained("output-llama3.3-qlora-cve")
tokenizer.save_pretrained("output-llama3.3-qlora-cve")














xxxxxx


import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments, DataCollatorForLanguageModeling
from peft import get_peft_model, LoraConfig, TaskType

# Model and tokenizer setup
model_name = "meta-llama/Llama-3.3-70B-Instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name, padding_side='right', use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set

# Load model efficiently
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    load_in_8bit=True,
    torch_dtype=torch.float16,
    device_map='auto'
)

# Apply LoRA with tuned parameters
peft_config = LoraConfig(
    task_type=TaskType.CAUSAL_LM,
    inference_mode=False,
    r=16,               # Slightly higher rank for better quality
    lora_alpha=32,
    lora_dropout=0.05,  # Reduced dropout for stability
    target_modules=['q_proj', 'v_proj']  # standard modules for llama family
)

model = get_peft_model(model, peft_config)
model.print_trainable_parameters()

# Dataset loading
dataset = load_dataset("euisuh15/cveFixes1", split="train")

# Define preprocess function explicitly handling padding
max_length = 2048  # Recommended to avoid token limit issues

def preprocess(example):
    prompt = (f"<|system|>\nYou are a code vulnerability injector.\n"
              f"<|user|>\nCWE: {example['cwe']}\n"
              f"Please inject the specified vulnerability into the following C code:\n\n```c\n{example['non_vulnerable_code']}\n```\n"
              f"<|assistant|>\n```c\n{example['vulnerable_code']}\n```")

    tokenized = tokenizer(
        prompt,
        truncation=True,
        max_length=max_length,
        padding='max_length'
    )

    labels = tokenized['input_ids'].copy()

    return {
        "input_ids": tokenized["input_ids"],
        "attention_mask": tokenized["attention_mask"],
        "labels": labels,
    }

tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)

# Data collator for padding
data_collator = DataCollatorForLanguageModeling(
    tokenizer=tokenizer, mlm=False
)

# Training arguments
training_args = TrainingArguments(
    output_dir="./llama3_cwe_finetuned",
    per_device_train_batch_size=1,
    gradient_accumulation_steps=16,  # Adjust based on GPU capacity
    num_train_epochs=3,
    learning_rate=1e-5,              # Lowered LR for stability
    fp16=True,
    logging_steps=10,
    save_steps=200,
    save_total_limit=3,
    evaluation_strategy="no",
    warmup_steps=50,
    weight_decay=0.01,
    gradient_checkpointing=True,     # Important for large models
    optim="adamw_torch"
)

# Initialize Trainer
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,
    data_collator=data_collator,
    tokenizer=tokenizer
)

# Start fine-tuning
trainer.train()

# Save final LoRA fine-tuned model
trainer.save_model("./llama3_cwe_finetuned_final")




**Preprocessing Function:**

```python
from datasets import load_dataset
from transformers import AutoTokenizer

# Load the 'c' split of the dataset
dataset = load_dataset("euisuh15/cveFixes1", split="c")

# Initialize tokenizer
tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3.3-70B-Instruct")
tokenizer.pad_token = tokenizer.eos_token  # Ensure padding token is set

def preprocess(example):
    prompt = (
        "<|system|>\nYou are a code vulnerability injector.\n"
        f"<|user|>\nCWE: {example['cwe_id']}\n"
        "Please inject the specified vulnerability into the following C code:\n\n"
        f"```c\n{example['code_before']}\n```\n"
        "<|assistant|>\n```c\n"
    )
    completion = f"{example['code_after']}\n```"

    # Tokenize prompt and completion
    prompt_tokens = tokenizer(prompt, truncation=True, max_length=2048, padding='max_length', return_tensors="pt")
    completion_tokens = tokenizer(completion, truncation=True, max_length=2048, padding='max_length', return_tensors="pt")

    # Combine input_ids and labels
    input_ids = torch.cat([prompt_tokens['input_ids'], completion_tokens['input_ids']], dim=1)
    attention_mask = torch.cat([prompt_tokens['attention_mask'], completion_tokens['attention_mask']], dim=1)
    labels = input_ids.clone()
    labels[:, :prompt_tokens['input_ids'].size(1)] = -100  # Mask prompt tokens in labels

    return {
        "input_ids": input_ids.squeeze(),
        "attention_mask": attention_mask.squeeze(),
        "labels": labels.squeeze(),
    }

# Apply preprocessing
tokenized_dataset = dataset.map(preprocess, remove_columns=dataset.column_names)











# Assuming we have a pre-loaded LLaMA 70B model and tokenizer accessible as `model` (pseudo-code).
# For instance, model could be a HuggingFace pipeline or similar interface.

def analyze_document(english_text: str) -> dict:
    """
    Analyze an English document to extract its style and content characteristics.
    Returns a dictionary with keys like 'tone', 'formality', 'rhetorical_style',
    'vocabulary_level', 'structure', and 'content_summary'.
    """
    # Construct an English prompt to instruct the model for analysis.
    analysis_prompt = (
        "You are a literary analyst. Analyze the following English text and describe its style and content characteristics.\n\n"
        "Text:\n"
        f"\"\"\"\n{english_text}\n\"\"\"\n\n"
        "Please provide an analysis with the following details:\n"
        "- Tone (e.g., optimistic, critical, scholarly, informal, etc.)\n"
        "- Formality level (formal or informal)\n"
        "- Rhetorical style and devices used (e.g., persuasive, uses analogies, rhetorical questions, humor)\n"
        "- Vocabulary level (simple, conversational, or advanced/technical)\n"
        "- Structure (how the text is organized, e.g., narrative, essay with introduction/body/conclusion, list, etc.)\n"
        "- A brief content summary of the text\n\n"
        "Format the analysis as follows:\n"
        "Tone: <description>\n"
        "Formality: <description>\n"
        "Rhetorical style: <description>\n"
        "Vocabulary level: <description>\n"
        "Structure: <description>\n"
        "Summary: <description>\n"
    )
    # Run a single inference to get the analysis.
    analysis_output = model.generate(analysis_prompt)  # Pseudo-call to the LLaMA model
    
    # Parse the output into a dictionary.
    analysis = {}
    for line in analysis_output.splitlines():
        if ": " in line:
            key, value = line.split(": ", 1)
            key = key.strip().lower().replace(" ", "_")  # e.g., "Tone" -> "tone"
            analysis[key] = value.strip()
    return analysis

# Example usage of analysis:
english_doc = "In an era of uncertainty, the only constant is change. The article explores how ... (etc.)"
analysis_result = analyze_document(english_doc)
print(analysis_result)
# Example output (dictionary):
# {
#  'tone': 'Reflective and motivational',
#  'formality': 'Semi-formal, conversational',
#  'rhetorical_style': 'Uses metaphors and rhetorical questions to engage the reader',
#  'vocabulary_level': 'Mix of common language with a few advanced terms',
#  'structure': 'Starts with an anecdotal introduction, followed by argumentative body paragraphs, and ends with a call-to-action conclusion',
#  'summary': 'The text discusses how change is a constant factor in times of uncertainty, encouraging adaptability and positive response to change.'
# }

def generate_in_chinese(analysis: dict) -> str:
    """
    Generate a Chinese version of the text using the style/content analysis.
    The prompt is in English but instructs the model to write in Chinese.
    Preserves the original text's style, tone, structure, and meaning,
    producing fluent Simplified Chinese output.
    """
    # Extract details from analysis
    tone = analysis.get('tone', '')
    formality = analysis.get('formality', '')
    rhet_style = analysis.get('rhetorical_style', '')
    vocab = analysis.get('vocabulary_level', '')
    structure = analysis.get('structure', '')
    summary = analysis.get('summary', '')
    # Construct the prompt
    prompt = (
        f"The original English text has the following characteristics:\n"
        f"Tone: {tone}\n"
        f"Formality: {formality}\n"
        f"Rhetorical style: {rhet_style}\n"
        f"Vocabulary level: {vocab}\n"
        f"Structure: {structure}\n"
        f"Summary of content: {summary}\n\n"
        "Using the information above, write a **Chinese** version of the text. "
        "The Chinese output should faithfully **preserve the tone, style, and structure** of the original. "
        "Write in natural, fluent Chinese (use Simplified Chinese characters) so that it reads as if originally written in Chinese. "
        "Maintain the same level of formality and include similar rhetorical devices (e.g., if the original posed a rhetorical question or used a metaphor, do the same in Chinese in an appropriate way). "
        "Ensure all key points from the summary are covered in detail, expanding the summary back into a full Chinese text. "
        "Output the final text in Chinese only, with no English explanation."
    )
    chinese_text = model.generate(prompt)  # Run the model to get Chinese text
    return chinese_text

def generate_in_arabic(analysis: dict) -> str:
    """
    Generate an Arabic version of the text using the style/content analysis.
    The prompt is in English but instructs the model to write in Arabic.
    Preserves the text's style and structure, producing fluent Modern Standard Arabic.
    """
    tone = analysis.get('tone', '')
    formality = analysis.get('formality', '')
    rhet_style = analysis.get('rhetorical_style', '')
    vocab = analysis.get('vocabulary_level', '')
    structure = analysis.get('structure', '')
    summary = analysis.get('summary', '')
    prompt = (
        f"The original English text has the following characteristics:\n"
        f"Tone: {tone}\n"
        f"Formality: {formality}\n"
        f"Rhetorical style: {rhet_style}\n"
        f"Vocabulary level: {vocab}\n"
        f"Structure: {structure}\n"
        f"Summary of content: {summary}\n\n"
        "Now write an **Arabic** version of the text, adhering to the above tone, formality, and style. "
        "The output should be in **Modern Standard Arabic**, fluent and culturally appropriate for Arabic readers. "
        "Preserve the original structure and rhetorical feel: for example, maintain any persuasive tone, emotional cues, or rhetorical questions in an Arabic context. "
        "Use Arabic vocabulary that matches the complexity of the original. "
        "Make sure the translation covers all the points from the summary with the same level of detail. "
        "Provide the final answer in Arabic only."
    )
    arabic_text = model.generate(prompt)
    return arabic_text

def generate_in_persian(analysis: dict) -> str:
    """
    Generate a Persian (Farsi) version of the text using the style/content analysis.
    The prompt is in English but instructs the model to write in Persian.
    Preserves the text's style and tone, producing fluent Persian output.
    """
    tone = analysis.get('tone', '')
    formality = analysis.get('formality', '')
    rhet_style = analysis.get('rhetorical_style', '')
    vocab = analysis.get('vocabulary_level', '')
    structure = analysis.get('structure', '')
    summary = analysis.get('summary', '')
    prompt = (
        f"Based on the following analysis of an English text:\n"
        f"Tone: {tone}\n"
        f"Formality: {formality}\n"
        f"Rhetorical style: {rhet_style}\n"
        f"Vocabulary level: {vocab}\n"
        f"Structure: {structure}\n"
        f"Summary: {summary}\n\n"
        "Please write a **Persian** version of the text that follows the same style and content. "
        "The output should be in Persian (Farsi) and should read naturally to a native speaker. "
        "Maintain the same tone and formality – for instance, if the original is formal and technical, use formal Persian wording at a similar complexity. "
        "Recreate any rhetorical devices in a way that makes sense in Persian (e.g. translate metaphors or analogies into culturally equivalent Persian expressions). "
        "Follow the same structure: the Persian text should have the corresponding introduction, body, and conclusion (or equivalent) as described. "
        "Ensure all key points from the summary are included. Output only the Persian text."
    )
    persian_text = model.generate(prompt)
    return persian_text

def generate_in_spanish(analysis: dict) -> str:
    """
    Generate a Spanish version of the text using the style/content analysis.
    The prompt is in English but instructs the model to write in Spanish.
    Preserves the text's style and tone, producing fluent Spanish output.
    """
    tone = analysis.get('tone', '')
    formality = analysis.get('formality', '')
    rhet_style = analysis.get('rhetorical_style', '')
    vocab = analysis.get('vocabulary_level', '')
    structure = analysis.get('structure', '')
    summary = analysis.get('summary', '')
    prompt = (
        f"An English text was analyzed with these characteristics:\n"
        f"Tone: {tone}\n"
        f"Formality: {formality}\n"
        f"Rhetorical style: {rhet_style}\n"
        f"Vocabulary level: {vocab}\n"
        f"Structure: {structure}\n"
        f"Summary: {summary}\n\n"
        "Now, rewrite the text in **Spanish**, preserving the above style and content. "
        "The Spanish version should maintain the same tone, level of formality, and structure. "
        "Ensure the language sounds natural in Spanish – use idiomatic expressions and phrasing that a native Spanish reader would expect. "
        "If the original text uses specific rhetoric (like an anecdote or a motivational tone), convey that in Spanish in an equivalent way. "
        "Use a Spanish vocabulary that matches the complexity of the original text. "
        "Re-expand the summary into a full, detailed Spanish text covering all main points. "
        "Provide only the Spanish translated text as output."
    )
    spanish_text = model.generate(prompt)
    return spanish_text

# Example usage of the pipeline:
analysis = analyze_document(english_doc)             # Analyze style/content once
chinese_output = generate_in_chinese(analysis)       # Generate Chinese translation
arabic_output = generate_in_arabic(analysis)         # Generate Arabic translation
persian_output = generate_in_persian(analysis)       # Generate Persian translation
spanish_output = generate_in_spanish(analysis)       # Generate Spanish translation








xxxxx


from peft import PeftModel
from transformers import AutoModelForCausalLM

# Load the base LLaMA-3 model and apply LoRA adapters
base_model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)
model = PeftModel.from_pretrained(base_model, "./qlora-llama3-cve")

# Optional: merge LoRA weights into base model for faster generation
model = model.merge_and_unload()

# Inference prompt
cve_id = "CVE-2014-0160"
non_vuln_code = """
static void tls1_process_heartbeat(TLS1_STATE *s) {
    unsigned char *p = &s->s3->rrec.data[0], *pl;
    unsigned short hbtype;
    unsigned int payload;

    hbtype = *p++;
    n2s(p, payload);
    ...
}
"""

prompt = f"""### Instruction:
Insert vulnerability CVE-ID {cve_id} into the following function:
{non_vuln_code}

### Response:
"""

# Tokenize and generate
inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
outputs = model.generate(
    **inputs,
    max_new_tokens=512,
    temperature=0.7,
    top_p=0.95,
    do_sample=True,
    eos_token_id=tokenizer.eos_token_id
)
response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:], skip_special_tokens=True)
print("--- Model Output ---\n", response)








xxxxx


Great — since you're using TRL v0.16.1 and want to fine-tune in instruction-response format only on `.c` code from the dataset, I’ll fix the dataset formatting bug and restrict training to C-language functions only.

I'll provide a corrected version of:
- Data loading and filtering for only `c` split
- A proper `formatting_func` compatible with TRL v0.16.1
- QLoRA-compatible training code for LLaMA 3 70B using PEFT and Hugging Face ecosystem

Hang tight, I’ll get this updated code and setup ready for you shortly.

# QLoRA Fine-tuning Pipeline for LLaMA-3 70B Instruct on CVEfixes1 'C' Dataset

In this script, we fine-tune Meta's **LLaMA-3 70B Instruct** model on the **CVEfixes1** dataset (using only the 'C' subset for C language functions) with the **QLoRA** technique. We will prepare the data in an **instruction-response** format as specified, apply a 90/10 train/validation split, and use Hugging Face's TRL **SFTTrainer** (v0.16.1) with PEFT LoRA for training. The code includes necessary formatting and checks to ensure compatibility with TRL v0.16.1 and robust handling of edge cases in the dataset.

## Setup and Imports

First, install and import the required libraries: Hugging Face Transformers, Datasets, BitsAndBytes (for 4-bit quantization), PEFT (for LoRA), and TRL (v0.16.1 for SFTTrainer). We also set the device and data type for computation. 

```python
!pip install transformers>=4.45.0 datasets bitsandbytes peft trl==0.16.1

import torch
from datasets import load_dataset
from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
from trl import SFTConfig, SFTTrainer, DataCollatorForCompletionOnlyLM

# Determine compute dtype: use bfloat16 if available (Ampere GPUs or higher), else float16
if torch.cuda.is_available():
    device_capability = torch.cuda.get_device_capability()
    # Ampere (SM>=80) or higher GPUs support bfloat16
    compute_dtype = torch.bfloat16 if device_capability[0] >= 8 else torch.float16
else:
    compute_dtype = torch.float16  # use float16 as fallback for CPU/offline training
print("Using compute dtype:", compute_dtype)
```

## Load and Prepare the CVEfixes1 'C' Dataset

We load the **CVEfixes1** dataset from Hugging Face and select only the **'c' split** (C language functions). We then filter out any entries with missing fields and perform a 90/10 train-validation split. This ensures we train on 90% of the data and reserve 10% for evaluation. 

```python
# Load the CVEfixes1 dataset and select only the 'c' split (C language functions)
dataset = load_dataset("euisuh15/cveFixes1")
c_dataset = dataset["c"]  # subset for C functions (2.62k examples)

# Filter out any records that don't have the required fields (cve_id, func_before, func_after)
def has_required_fields(example):
    return example.get("cve_id") and example.get("func_before") and example.get("func_after")

c_dataset = c_dataset.filter(has_required_fields)

# Shuffle and split the dataset into train (90%) and validation (10%)
c_dataset = c_dataset.shuffle(seed=42)
split_datasets = c_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = split_datasets["train"]
eval_dataset = split_datasets["test"]

print(f"Train examples: {len(train_dataset)}, Validation examples: {len(eval_dataset)}")
# Inspect a sample to verify presence of fields
sample = train_dataset[0]
print("Sample keys:", sample.keys())
print("CVE ID:", sample["cve_id"])
print("func_after snippet:", sample["func_after"][:60], "...")
print("func_before snippet:", sample["func_before"][:60], "...")
```

## Define Formatting Function for Instruction-Response Data

Next, we create a formatting function to convert each dataset record into the **instruction-response** prompt format required. For each example:
- **Instruction**: `"Insert vulnerability CVE-ID {cve_id} into the following function:"` followed by the `func_after` code (the fixed version of the function).
- **Response**: The corresponding `func_before` code (the vulnerable function before the fix).
- We prefix the instruction with `### Instruction:` and the response with `### Response:`. The final formatted text will look like: 

  ```
  ### Instruction:
  Insert vulnerability CVE-ID <ID> into the following function:
  <func_after code>
  
  ### Response:
  <func_before code>
  ```
- We also **append an EOS token** at the end of each sequence to explicitly mark the end of the response. This is important because LLaMA-3's tokenizer may not add an EOS by default, and including it helps the model learn when to stop ([How to set the Pad Token for meta-llama/Llama-3 Models - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418#:~:text=If%20you%20want%20to%20fine,be%20correct%20at%20inference%20time)).

Finally, we set up a data collator from TRL (`DataCollatorForCompletionOnlyLM`) with the instruction and response templates. This collator will ensure that only the response part contributes to the training loss (it masks out the instruction tokens) so the model learns to predict the response given the instruction ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=You%20can%20use%20the%20,how%20it%20would%20work%20to)).

```python
# Load the LLaMA-3 Instruct tokenizer and model (we'll configure the model in the next section)
model_name = "meta-llama/Llama-3.3-70B-Instruct"  # or "Meta-Llama-3-70B-Instruct" depending on availability
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True, trust_remote_code=True)

# Ensure the tokenizer has a padding token set. 
# LLaMA models don't have a pad token by default; we'll use a reserved special token for padding if available.
if tokenizer.pad_token_id is None:
    try:
        tokenizer.pad_token = "<|finetune_right_pad_id|>"  # Use reserved pad token if present ([How to set the Pad Token for meta-llama/Llama-3 Models - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418#:~:text=If%20you%20want%20to%20fine,be%20correct%20at%20inference%20time))
    except Exception as e:
        tokenizer.pad_token = tokenizer.eos_token  # Fallback: use EOS as pad (not ideal for training ([How to set the Pad Token for meta-llama/Llama-3 Models - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418#:~:text=If%20you%20want%20to%20fine,be%20correct%20at%20inference%20time)))
    tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)
print("Pad token set to:", tokenizer.pad_token, "(ID:", tokenizer.pad_token_id, ")")

# Define the formatting function for each example
def format_example(example):
    cve_id = example["cve_id"]
    func_after = example["func_after"]
    func_before = example["func_before"]
    # Construct instruction and response strings
    instruction = f"Insert vulnerability CVE-ID {cve_id} into the following function:\n{func_after}"
    response = func_before
    formatted = f"### Instruction:\n{instruction}\n\n### Response:\n{response}"
    # Append EOS token to mark end of response (important for LLaMA-3) ([How to set the Pad Token for meta-llama/Llama-3 Models - Models - Hugging Face Forums](https://discuss.huggingface.co/t/how-to-set-the-pad-token-for-meta-llama-llama-3-models/103418#:~:text=If%20you%20want%20to%20fine,be%20correct%20at%20inference%20time))
    eos = tokenizer.eos_token
    if eos and not formatted.endswith(eos):
        formatted += eos
    return formatted

# Example formatting (print a single example to verify format)
formatted_text = format_example(sample)
print("--- Formatted example ---")
print(formatted_text[:200], "...\n")  # print first 200 characters for brevity

# Set up the data collator to mask everything except the response during training
instruction_template = "### Instruction:\n"
response_template = "### Response:\n"
collator = DataCollatorForCompletionOnlyLM(
    instruction_template=instruction_template,
    response_template=response_template,
    tokenizer=tokenizer,
    mlm=False
)
```

## Initialize LLaMA-3 70B Model with 4-bit Quantization

Now we load the **LLaMA-3 70B Instruct** model with 4-bit precision (using BitsAndBytes for NVidia's 4-bit quantization). This significantly reduces memory usage, making fine-tuning feasible (the essence of QLoRA). We specify `bnb_4bit_quant_type="nf4"` (normal float4) and enable double quantization for better accuracy. We also use `device_map="auto"` to automatically distribute the model layers across available GPUs (or CPU if needed).

After loading, we call `prepare_model_for_kbit_training` from PEFT, which **enables gradient checkpointing and disables a few modules' gradients** to optimize 4-bit training (e.g., it keeps norm layers in FP32). This prepares the model for LoRA fine-tuning in 4-bit ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=model%20%3D%20FastLanguageModel,gate_proj)).

```python
# Configure 4-bit quantization for QLoRA
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,
    bnb_4bit_quant_type="nf4",
    bnb_4bit_compute_dtype=compute_dtype  # use the compute dtype determined earlier (bf16/fp16)
)

# Load the base LLaMA-3 70B Instruct model in 4-bit mode
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",
    trust_remote_code=True
)

# Prepare model for 4-bit training (enables grad checkpointing, etc.)
model = prepare_model_for_kbit_training(model)
model.gradient_checkpointing_enable()  # Ensure gradient checkpointing is on for memory savings
```

## Configure LoRA (QLoRA) Parameters

Next, we configure the **LoRA** (Low-Rank Adaptation) parameters for QLoRA. We use PEFT's `LoraConfig` to specify:
- `r = 16`: LoRA rank (projection dimension).  
- `lora_alpha = 16`: LoRA scaling factor.
- `target_modules`: the list of model sub-modules to apply LoRA to. For LLaMA, we target the transformer projection layers in attention (`q_proj`, `k_proj`, `v_proj`, `o_proj`) and feed-forward (`gate_proj`, `up_proj`, `down_proj`) ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=model%20%3D%20FastLanguageModel,gate_proj)). This covers all major weight matrices, as recommended for LLaMA fine-tuning.
- `lora_dropout = 0.0`: No dropout in LoRA (often 0 is used for stability).
- `bias = "none"`: We do not train any bias terms.
- `task_type = "CAUSAL_LM"`: This sets up LoRA for a causal language modeling task.

We then wrap the base model with `get_peft_model` to inject LoRA adapters. This will add trainable LoRA layers to the model while keeping all original model weights frozen (since we called `prepare_model_for_kbit_training`, most weights require grad = False). We also print out the number of trainable vs total parameters to verify that only a small fraction are trainable.

```python
# Configure LoRA (Low-Rank Adaptation) parameters for QLoRA
lora_config = LoraConfig(
    r=16,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.0,
    bias="none",
    task_type="CAUSAL_LM"
)

# Wrap the base model with PEFT LoRA
model = get_peft_model(model, lora_config)

# Optional: Print trainable parameter ratio for verification
trainable_params = 0
total_params = 0
for name, param in model.named_parameters():
    total_params += param.numel()
    if param.requires_grad:
        trainable_params += param.numel()
print(f"Trainable params: {trainable_params} / {total_params} ({100 * trainable_params/total_params:.2f}% of total)")
```

## Prepare SFT Trainer for Fine-Tuning

With the model, data, and formatting ready, we set up the **SFTTrainer** from TRL. We create an `SFTConfig` (which extends Hugging Face `TrainingArguments`) to specify training hyperparameters and output settings:
- `output_dir`: where to save the model checkpoints.
- `max_length`: maximum sequence length for training. We use 1024 tokens to truncate or pad sequences longer than this (adjustable if memory allows longer contexts) ([trl/trl/trainer/sft_config.py at main · huggingface/trl · GitHub](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_config.py#:~:text=max_length%20%28%60int%60%20or%20%60None%60%2C%20,1024)).
- `num_train_epochs`: we set a few epochs (e.g., 3). Given the small dataset (≈2.3k train examples), multiple epochs can help the model see all data.
- `per_device_train_batch_size`: set to 1 (common for very large models due to memory constraints).
- `gradient_accumulation_steps`: set to 8 so that the effective batch size is 8 (this accumulates gradients over 8 steps before an optimizer step).
- `learning_rate`: we set a learning rate (e.g., 2e-4) suitable for LoRA fine-tuning. (TRL's default is 2e-5 ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=,TrainingArguments)), but we can use a higher LR for a small dataset; this can be tuned).
- `warmup_ratio`: a 3% linear warmup to gradually ramp up the learning rate at start.
- `evaluation_strategy = "epoch"` and `save_strategy = "epoch"`: evaluate on the validation set and save the model at the end of each epoch.
- `logging_steps`: how frequently to log training info; we set to 50 for example.

We then initialize the `SFTTrainer` with the model, datasets, tokenizer, our `format_example` function, and the data collator. The trainer will automatically format each example using `format_example`, tokenize it, and apply the collator to prepare batches. Finally, we start training with `trainer.train()`.

```python
# Configure training arguments for SFTTrainer
training_args = SFTConfig(
    output_dir="./qlora-llama3-cve",   # output directory for model checkpoints
    max_length=1024,                  # max sequence length (truncates sequences longer than this) ([trl/trl/trainer/sft_config.py at main · huggingface/trl · GitHub](https://github.com/huggingface/trl/blob/main/trl/trainer/sft_config.py#:~:text=max_length%20%28%60int%60%20or%20%60None%60%2C%20,1024))
    num_train_epochs=3,
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,    # accumulate gradients to simulate larger batch
    learning_rate=2e-4,
    warmup_ratio=0.03,                # 3% of training steps for warmup
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    save_total_limit=1,              # keep only the most recent checkpoint
    logging_strategy="steps"
)

# Initialize the SFTTrainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    tokenizer=tokenizer,
    formatting_func=format_example,       # apply our formatting to each dataset example
    data_collator=collator               # collator that masks instruction tokens
)

# Start the training
trainer.train()

# After training, the LoRA-adapted model will be saved in output_dir.
# (The final model includes the base model with LoRA weights. You can also separately save LoRA adapters if needed.)
```








xxxx 



Thanks for the image — the traceback shows this error in your `format_example` function:

```
TypeError: string indices must be integers, not 'str'
```

### Root Cause:
This means you're trying to index into a **string** as if it were a **dictionary**. Specifically, the error is at this line:
```python
return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
        f"{example['func_after']}\n\n### Response:\n{example['func_before']}")
```

The issue is that `example` is actually a **string**, not a dictionary — likely because you're passing a **batched string** to the function instead of an object like `{"cve_id": ..., "func_after": ..., "func_before": ...}`.

---

### Fix:

Update your `formatting_prompts_func` like this:

```python
def formatting_prompts_func(example):
    return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
            f"{example['func_after']}\n\n### Response:\n{example['func_before']}")
```

Then **remove the batching**. Pass this function as `formatting_func`, not wrapped in a list comprehension.

Also, make sure you're not wrapping it again in a batch:

```python
# No list comprehension needed
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collator,
    formatting_func=formatting_prompts_func,  # NOT a batched version
    args=sft_config
)
```

If the `SFTTrainer` expects a batched version (which depends on your TRL version), you can instead add a check:

```python
def formatting_prompts_func(example_or_batch):
    if isinstance(example_or_batch, dict) and 'cve_id' in example_or_batch:
        return (f"Insert vulnerability CVE-ID {example_or_batch['cve_id']} into the following function:\n"
                f"{example_or_batch['func_after']}\n\n### Response:\n{example_or_batch['func_before']}")
    elif isinstance(example_or_batch, list):
        return [formatting_prompts_func(ex) for ex in example_or_batch]
    else:
        raise ValueError("Invalid input format for formatting function")
```

This version is robust and handles both single examples and batches.

Let me know if you want help testing it live or checking token lengths per example.






# Now that tokenizer is available, create the data collator for completion-only language modeling
collator = DataCollatorForCompletionOnlyLM(
    response_template="### Response:",
    tokenizer=tokenizer,
    mlm=False  # not doing masked language modeling, this is causal LM
)

# Define training arguments via SFTConfig (which is similar to HF TrainingArguments)
sft_config = SFTConfig(
    model_name_or_path=None,  # not needed since we pass the model object to trainer
    output_dir="./llama3-70b-vuln-qlora", 
    num_train_epochs=3,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",          # cosine decay schedule (just an example choice)
    warmup_steps=100,                    # brief warmup for stability
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,       # accumulate gradients to simulate larger batch
    optim="paged_adamw_8bit",            # use memory-efficient 8-bit AdamW optimizer&#8203;:contentReference[oaicite:26]{index=26}
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    bf16=True,        # use bfloat16 precision for training (if supported by your GPU, else use fp16)
    tf32=True         # enable TF32 on Ampere GPUs for faster math (optional)
)

# Initialize the SFT Trainer
trainer = SFTTrainer(
    model=model, 
    train_dataset=train_dataset, 
    eval_dataset=eval_dataset,
    data_collator=collator,
    formatting_func=formatting_prompts_func,  # apply our prompt formatting
    args=sft_config
)

# Train the model!
trainer.train()




# Evaluate on validation set
eval_results = trainer.evaluate()
val_loss = eval_results["eval_loss"]
print(f"Validation loss: {val_loss:.4f}")
try:
    import math
    print(f"Validation perplexity: {math.exp(val_loss):.2f}")
except OverflowError:
    print("Validation perplexity is inf (loss might be too high or not applicable).")







xxxxx


Thanks for the details. I’ll prepare the data processing pipeline and fine-tuning code for using QLoRA/PEFT to train a LLaMA 3 70B Instruct model where the model learns to inject specific CVEs into code. I'll structure the dataset so that:

- **Input**: Instruction to insert a given CVE ID into the code, followed by the non-vulnerable code (`function_after`).
- **Output**: Vulnerable version of the code (`function_before`).

I'll include:
- Code to load and preprocess the dataset
- Format each sample into instruction-tuning format compatible with LLaMA 3
- Split into train and validation (90/10)
- Fine-tuning code using PEFT/QLoRA

I’ll let you know once everything is ready.

# Fine-tuning LLaMA 3 70B with QLoRA for Vulnerability Injection

## Introduction and Overview

In this guide, we will fine-tune Meta's **LLaMA 3 70B Instruct** model (70 billion parameters) to perform a specialized code transformation task: **inserting vulnerabilities** (based on specific CVE IDs) into given code functions. The training data comes from the *CVEfixes1* dataset, which contains thousands of real-world vulnerability-fixing code changes (before-and-after code for over 5,000 CVEs) ([CVEfixes Dataset: Automatically Collected Vulnerabilities and Their Fixes from Open-Source Software](https://zenodo.org/records/4476564#:~:text=organized%20as%20a%20relational%20database,18249%20files%2C%20and%2050322%20functions)). We will use **instruction tuning**: each training sample is a prompt instructing the model to inject a particular CVE vulnerability into a piece of code, and the expected output is the code with that vulnerability injected.

Because a 70B model is extremely large, we employ **QLoRA (Quantized Low-Rank Adaptation)** ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)) in combination with Hugging Face's **PEFT (Parameter-Efficient Fine-Tuning)** framework. QLoRA allows fine-tuning a 70B model on a single or few GPUs by: (a) loading the base model in 4-bit precision (to drastically reduce memory), and (b) attaching small Low-Rank Adapter (LoRA) weights which are the only trainable parameters ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,innovations%20to%20save%20memory%20without)). The base model remains frozen (in 4-bit quantized form) and gradients update only the LoRA layers. This approach preserves nearly full fine-tuning performance while using a fraction of the memory of full 16-bit training ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)). We will also enable **gradient checkpointing** (to save memory by recomputing activations), and take advantage of **FlashAttention** for faster training if available ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=pip%20install%20)). Model parallelism (distributing the model across GPUs) will be handled automatically by Hugging Face Accelerate's `device_map`, since a 70B model in 4-bit may still need multiple devices.

**Key steps in this tutorial:**

- **Data Preparation:** Load the CVE fixes dataset, combine code snippets, and format each example as an instruction-response pair (with a 90/10 train/validation split).
- **Model Loading (LLaMA 3 70B Instruct):** Load the LLaMA 3 70B instruct model and tokenizer in 4-bit precision using `bitsandbytes`, verifying we use LLaMA 3's new tokenizer (with expanded vocabulary) ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the)) and 8k token context window ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=The%20Llama%203%20release%20introduces,context%20length%20of%208K%20tokens)).
- **QLoRA Setup:** Prepare the model for 4-bit training (casting certain layers to float32 for stability) and add LoRA adapters to the appropriate layers (e.g. query/key projection layers of the Transformer).
- **Training Configuration:** Define hyperparameters and a training loop using Hugging Face Transformers + TRL. We'll use the `SFTTrainer` from 🤗 TRL (Transformer Reinforcement Learning library) for supervised fine-tuning, along with a special data collator that ensures only the *response* portion of each sample is used for computing the loss (so the model is trained to predict the vulnerable code given the prompt).
- **Optimization for 70B:** Enable gradient checkpointing, use a memory-efficient optimizer (8-bit AdamW ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))】, and optionally FlashAttention 2 for faster attention computatio ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=pip%20install%20))】.
- **Running Training & Evaluation:** Train the model on the training set and evaluate on the held-out validation set. We will demonstrate how to save the LoRA adapter and how to use the fine-tuned model to generate vulnerable code given a new instruction.

Throughout this guide, we include well-documented code snippets and explanations for each step.

## 1. Setup and Dataset Preparation

First, install and import the required libraries: Hugging Face 🤗 Transformers, Datasets, PEFT, TRL, and BitsAndBytes (for 4-bit quantization). Ensure you have a compatible GPU with sufficient memory (multiple GPUs may be needed for a 70B model). Then load the **cveFixes1** dataset from Hugging Face. This dataset contains code **before** and **after** fixing a vulnerability, along with the CVE ID and other metadata. We will combine all programming languages in the dataset and then split into 90% train and 10% validation.

```python
!pip install transformers accelerate peft trl bitsandbytes datasets

from datasets import load_dataset

# Load the CVEfixes1 dataset. It has splits by language; we combine them.
dataset = load_dataset("euisuh15/cveFixes1")  # returns a dict of splits: 'c', 'go', 'java', 'python', 'ruby'

# Concatenate all language splits into one dataset
all_splits = [dataset[lang] for lang in dataset.keys()]  # dataset.keys() -> ['c','go','java','python','ruby']
full_dataset = all_splits[0].concatenate(*all_splits[1:])

print(f"Total examples (all languages): {len(full_dataset)}")  # Expect ~4380 examples in total

# Shuffle and split 90/10 into train and validation sets
full_dataset = full_dataset.shuffle(seed=42)
train_val = full_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = train_val["train"]
eval_dataset = train_val["test"]

print(f"Train examples: {len(train_dataset)}; Validation examples: {len(eval_dataset)}")
```

**Explanation:** The dataset provides code in several languages (C, Java, Python, etc.), each containing functions *before* and *after* a vulnerability fix. By combining them, we get around 4.3K function pairs. We then shuffle and split the data into a training set (≈90%) and a validation set (≈10%). This ensures the model is evaluated on code it hasn't seen during training.

Each dataset record has fields like `func_before` (the vulnerable function), `func_after` (the fixed function), and `cve_id` (the identifier of the vulnerability). For our fine-tuning, we only need these three fields:
- **`cve_id`**: e.g., "CVE-2014-0160"
- **`func_after`**: the function *after* the fix (i.e., secure code).
- **`func_before`**: the function *before* the fix (i.e., vulnerable code).

## 2. Formatting Data for Instruction Tuning

We will format each training example as an **instruction** and an **expected response**. The instruction will prompt the model to insert a specific vulnerability into a given function. According to the specification:

- **Prompt (Instruction)**: *"Insert vulnerability CVE-ID `{cve_id}` into the following function:"* followed by the **`func_after`** code (the non-vulnerable code).
- **Expected Output (Response)**: The corresponding **`func_before`** code (the vulnerable version of the function).

To clearly separate the instruction from the model's response during training, we will insert a special marker **"### Response:"** before the vulnerable code. This marker will help our training collator distinguish the prompt from the completion. (This follows a common format in instruction tuning where prompts and responses are separated by tokens like `"### Response: ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=For%20instruction%20fine,This%20allows%20people%20to))1】.) For example, a formatted sample might look like:

```
Insert vulnerability CVE-ID CVE-2014-0160 into the following function:
<secure function code here>

### Response:
<vulnerable function code here>
```

We now prepare a formatting function and a data collator to apply this template to our dataset during training:

```python
from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM

# Define a formatting function to combine fields into a single prompt-response string
def format_example(example):
    return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
            f"{example['func_after']}\n\n### Response:\n{example['func_before']}")

# Wrap the formatting function to work on lists of examples for efficiency (vectorized mapping)
def formatting_prompts_func(batch):
    return [format_example(ex) for ex in batch]

# Initialize a data collator that will mask out the prompt portion in the loss.
tokenizer = None  # We will load the tokenizer later after model is loaded.
# We'll create the collator after initializing the tokenizer and model, because it needs the tokenizer.
```

**Explanation:** The `format_example` function constructs the instruction and response text. We include two newline characters before `"### Response:"` to ensure the response starts on a new line for clarity. During fine-tuning, the model will see the full text up to `"### Response:"` as the input prompt, and it should learn to produce the text *after* `"### Response:"` (the vulnerable code). We will use Hugging Face TRL's `DataCollatorForCompletionOnlyLM` to handle tokenization and labeling. This collator will tokenize the combined text, find the `"### Response:"` marker, and set the **labels** such that all tokens in the prompt (everything before and including the `"### Response:"` marker) are **ignored (label -100)**, and only the tokens of the response (the vulnerable code) are **predicted by the model ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=For%20instruction%20fine,This%20allows%20people%20to)) ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=You%20can%20use%20the%20,how%20it%20would%20work%20to))9】. This way, the loss is computed only on the model's output portion.

We haven't loaded the tokenizer or model yet, so we left the tokenizer as `None` for now. In the next step, we'll load the LLaMA 3 model and tokenizer, then finalize the collator.

## 3. Loading LLaMA 3 70B Instruct Model with 4-bit Quantization (QLoRA)

Now let's load the pre-trained **LLaMA 3 70B Instruct** model from Hugging Face. We use the instruct variant of LLaMA 3 70B (which has already been fine-tuned by Meta for following instruction ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=The%20Llama%203%20release%20introduces,context%20length%20of%208K%20tokens))8】 as our base model. It's important to use the correct **tokenizer** for LLaMA 3, since LLaMA 3 uses a new tokenizer with a much larger vocabulary (128k tokens vs 32k in LLaMA  ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the))4】. This tokenizer is needed to properly encode code and text for the model.

Because the model is so large, we'll load it in 4-bit precision using **BitsAndBytes** (via `quantization_config`). We also disable `use_cache` and enable gradient checkpointing to reduce memory usage during training. Gradient checkpointing will force the model to recompute certain layers on the fly instead of storing all intermediate activations, trading compute for memory – a necessary trade-off for 70B models.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

model_name = "meta-llama/Meta-Llama-3-70B-Instruct"  # placeholder HF model name for LLaMA3 70B instruct

# Define 4-bit quantization configuration (using NF4 quantization, per QLoRA pap ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4】)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,   # double quantization to reduce memo ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4】
    bnb_4bit_quant_type="nf4",        # NormalFloat4 (NF4) quantizati ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4】
    bnb_4bit_compute_dtype=torch.bfloat16  # use bfloat16 for computation (or float16 if bfloat16 not available)
)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# Note: use_fast=False because LLaMA tokenizer is usually implemented as a slow tokenizer. 
# Ensure the tokenizer is the one for LLaMA3 to get the 128k voc ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the))4】.

# Load the model in 4-bit mode across available devices
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",    # automatically split layers across GPUs (or CPU if needed)
    use_cache=False,      # disable cache for gradient checkpointing
    trust_remote_code=True  # if the model repo requires custom code (may not be needed for LLaMA3)
)

# Prepare model for 4-bit training
model.gradient_checkpointing_enable()  # enable gradient checkpointing for memory savings
model = prepare_model_for_kbit_training(model)
```

**Explanation:** We use `AutoTokenizer` and `AutoModelForCausalLM` to load the tokenizer and model. The `BitsAndBytesConfig` sets the model to load in 4-bit quantized mode (with NF4 quantization and double quantizatio ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4】. The `device_map="auto"` will automatically distribute the model's layers across all available GPUs (and CPU memory if necessary) using Hugging Face Accelerate. This is a form of model parallelism that is essential for handling a 70B model. For example, if you have 2×40GB GPUs, it might put roughly half the model on each GPU. If you only have a single GPU with enough memory (e.g., 48GB), it will put the whole model there. If the model still doesn't fit, some layers could be sharded to CPU RAM (which slows down training). You can adjust the `device_map` or use more advanced parallelism (like Fully Sharded Data Parallel) if needed, but `"auto"` is a convenient starting point.

We set `use_cache=False` because the model’s key/value cache is not needed during training and disabling it is required when using gradient checkpointing to avoid inconsistencies. We explicitly enable gradient checkpointing via `model.gradient_checkpointing_enable()`, and call `prepare_model_for_kbit_training(model)`. This utility from PEFT adjusts the model for low-bit training: it casts certain layers (e.g., layer norms and output embeddings) to FP32 for stability, and may handle some device placement tweaks for int8/4-bit traini ([Optimizing Language Model Fine-Tuning with PEFT, QLORA ...](https://medium.com/@tejpal.abhyuday/optimizing-language-model-fine-tuning-with-peft-qlora-integration-and-training-time-reduction-04df39dca72b#:~:text=Optimizing%20Language%20Model%20Fine,initializes%20the%20model%20for))7】. At this point, the 70B model is loaded in memory in 4-bit form, but we have not added our LoRA adapters yet.

**Optional – Flash Attention:** If you have the [FlashAttention 2](https://github.com/HazyResearch/flash-attention) library installed, you can **speed up attention computation** and reduce memory usage further. Simply install the library (`pip install flash-attn`) and when loading the model, pass `attn_implementation="flash_attention_2"` to `from_pretraine ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=To%20use%20Flash%20Attention%202%2C,attn%60%20package))9】. This uses optimized CUDA kernels for self-attention (supported on NVIDIA GPUs with SM>=70, e.g., A100, V100). FlashAttention 2 can be used with quantization and checkpointing for additional efficien ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=Note%20that%20Flash%20Attention%20only,other%20tools%20such%20as%20quantization)) ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=In%20contrast%20to%20Flash%20Attention,that%20also%20includes%20padding%20tokens))0】. If not installed, the model will use the default PyTorch implementation (which on PyTorch 2.x may already leverage FlashAttention under the hood for some operations).

## 4. Applying LoRA Adapters with PEFT

With the base model loaded and prepared, the next step is to configure LoRA adapters. LoRA will inject trainable weight matrices into the model's attention (and/or feed-forward) layers. Given the standard practice and QLoRA's recommendations, we will target the **query (`q_proj`) and value (`v_proj`) projection matrices** in each Transformer attention block for LLaMA. This keeps the number of trainable parameters small while still being effective at modeling new behavior (since Q and V are crucial for attention dynamics).

We choose a LoRA **rank** of, say, 8 or 16 (this controls the dimensionality of the low-rank updates; higher rank means more capacity but more VRAM usage). We also set a **LoRA scaling (alpha)**, often set to 16 or 32, which is a factor that scales the LoRA updates. We will not train any new biases (setting `bias="none"`). These hyperparameters can be tuned; here we use rank=8 and alpha=16 for demonstration, which yields a very lightweight adapter.

Let's attach LoRA to the model:

```python
from peft import LoraConfig, get_peft_model

# Configure LoRA
lora_config = LoraConfig(
    r=8,             # LoRA rank (increased for more capacity if needed, e.g., 16 or 32)
    lora_alpha=16,   # LoRA scaling factor
    target_modules=["q_proj", "v_proj"],  # which sub-modules to apply LoRA to (LLaMA's attention projections)
    bias="none", 
    task_type="CAUSAL_LM"  # type of task
)

# Wrap the model with PEFT LoRA
model = get_peft_model(model, lora_config)

# Print the number of trainable parameters to verify we are tuning a small subset
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {trainable_params} (~{100 * trainable_params/total_params:.4f}% of total)")
```

**Explanation:** We create a `LoraConfig` specifying how to inject the LoRA layers. By targeting `"q_proj"` and `"v_proj"` in each transformer block, we








xxxxxxx

Thanks! Since you're using 2 A100 GPUs and want to fine-tune the LLaMA 3 70B Instruct model with QLoRA via PEFT, I’ll put together updated training code that:

- Loads the 70B model in 4-bit with BitsAndBytes
- Uses Hugging Face PEFT to add LoRA adapters
- Sets up `TrainingArguments` with `gradient_accumulation`, `fp16`, and multi-GPU support
- Handles tokenizer, data collation, and masking properly for causal language modeling

I’ll include recommendations for LoRA config tuned for 70B (e.g., `r=64`, `target_modules='all-linear'`, `lora_dropout=0.1`) and how to train with Hugging Face Trainer.

I’ll get that code and guidance ready for you shortly.

# QLoRA Fine-tuning Example for LLaMA 3 70B Instruct

In this example, we show how to fine-tune a **LLaMA-3 70B Instruct** model on a small CVE vulnerability injection dataset using **QLoRA** (Quantized LoRA) with Hugging Face Transformers and PEFT. The code below loads the 70B model in 4-bit precision, adds LoRA adapters to all linear layers, prepares the dataset of prompt-target pairs, and runs efficient multi-GPU training on 2×A100 GPUs.

## 1. Setup and Model Loading (4-bit Quantization)

First, install the required libraries if you haven't already (Transformers, PEFT, BitsAndBytes, Accelerate):

```bash
pip install transformers accelerate bitsandbytes peft
```

Now we load the 70B model in **4-bit precision** using `bitsandbytes`. We use `device_map="auto"` to automatically shard the model across the two GPUs, and configure 4-bit **NF4** quantization with double quantization for efficiency ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig)). We also load the corresponding tokenizer and set the padding token:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "<path-or-name-of-llama3-70b-instruct>"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",         # NormalFloat4 quantization ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig))
    bnb_4bit_use_double_quant=True,    # Use double quantization for memory save ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig))
    bnb_4bit_compute_dtype=torch.bfloat16  # Use BF16 for computation on A100 ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))
)

# Load the tokenizer and model in 4-bit, sharding across GPUs
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to EOS (common practice for LLaMA)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",    # Automatically distribute layers across available GPUs ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20AutoModelForCausalLM))
    use_auth_token=True   # if model is behind authentication (optional)
)
```

> **Note:** Loading with `device_map="auto"` will place different layers of the 70B model on each GPU, enabling one process to utilize both GPUs as shared memory for the model ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20AutoModelForCausalLM)). This means you do **not** need to launch multiple processes for distributed training in this setup. The model remains in 4-bit quantized mode (NF4) but computations use higher precision (BF16) for accuracy ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)).

## 2. Configuring LoRA with PEFT

Next, we configure the LoRA parameters using the PEFT library. We apply LoRA to **all linear layers** of the model (Q, K, V, O in self-attention and the gated feed-forward projections) as recommended by QLoRA ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)). We'll use LoRA rank `r=64` and set `lora_alpha=16` (scaling factor) following the QLoRA paper settings ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)). The LoRA dropout is set to `0.1` (10%), which is a typical value for models up to 13B ([LoRA Fine-tuning & Hyperparameters Explained (in Plain English) | Entry Point AI](https://www.entrypointai.com/blog/lora-fine-tuning/#:~:text=The%20QLoRA%20paper%20set%20dropout,for%2033B%20and%2065B%20models)) (for 70B, the QLoRA paper suggests 0.05, but we use 0.1 as an example). We target all transformer linear layer names (`q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj`) for LoRA injection:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",           # We will not tune biases
    task_type="CAUSAL_LM"  # Language model fine-tuning
)
# Wrap the base model with LoRA adapters
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # Optional: verify number of trainable params vs total
```

This will add trainable LoRA weight matrices to each target linear layer while keeping the original 70B weights frozen. The `print_trainable_parameters()` should report a small fraction of total parameters as trainable (only the LoRA layers). For example, with r=64 on all linear layers, the trainable parameter count is on the order of a few hundred million (which is tiny relative to 70B).

## 3. Preparing the Dataset and Tokenization

Assume we have a Hugging Face `Dataset` of examples, each with a `"prompt"` (the fixed code + CVE description) and a `"target"` (the vulnerable code). We need to combine these into a single input sequence for the model, and mask the prompt part in the labels so that the loss is only computed on the target portion. 

We will concatenate the prompt and target with an end-of-sequence token, tokenize them, and set the labels such that tokens corresponding to the prompt are `-100` (ignored in loss calculation). This ensures a causal language modeling setup where the model is trained to predict only the vulnerable code given the prompt.

```python
from datasets import Dataset  # or DatasetDict if you have train/val

# Example: assume `raw_dataset` is your Dataset with 'prompt' and 'target' columns.
# If you have a DatasetDict with a train split, use raw_datasets["train"] accordingly.
raw_dataset = Dataset.from_dict({
    "prompt": ["<example prompt 1>", "<example prompt 2>"], 
    "target": ["<example vulnerable code 1>", "<example vulnerable code 2>"]
})
# Define a preprocessing function to tokenize and mask labels
def preprocess_example(example):
    prompt_text = example["prompt"]
    target_text = example["target"]
    # Concatenate prompt and target, with an EOS token at the end of target
    full_text = prompt_text + target_text + tokenizer.eos_token
    tokens = tokenizer(full_text, truncation=True)  # you can add max_length if needed
    input_ids = tokens["input_ids"]
    # Create labels copying input_ids, then mask prompt portion
    labels = input_ids.copy()
    prompt_len = len(tokenizer(prompt_text, add_special_tokens=False)["input_ids"])
    # Mask all prompt tokens (and padding if any) in the labels
    for i in range(prompt_len):
        labels[i] = -100
    # (Optional) also mask padding tokens just in case
    # But here we didn't add padding yet; will handle padding in collator.
    return {"input_ids": input_ids, "attention_mask": tokens["attention_mask"], "labels": labels}

# Apply preprocessing to the entire dataset
tokenized_dataset = raw_dataset.map(preprocess_example, remove_columns=raw_dataset.column_names)
```

**Data formatting tips:** Make sure each `prompt` already contains any necessary formatting that the model expects (for example, special tokens or separators between the fixed code and CVE description, if needed for clarity). In this simple example, we directly concatenate them. The model's EOS token is appended to mark the end of the response, which helps the model learn when to stop. By setting prompt tokens' labels to `-100`, we ensure the loss only comes from predicting the `target` (vulnerable code), not from regurgitating the prompt.

## 4. Setting Up Training Arguments

We use the Hugging Face `Trainer` API for convenience. Here are some important training hyperparameters and configurations for 2×A100 GPUs:

- **Batch Size & Gradient Accumulation:** We set `per_device_train_batch_size=1` (one sequence per GPU at a time, given the model size) and use `gradient_accumulation_steps` to accumulate gradients over multiple steps for a larger effective batch. For example, with 8 accumulation steps, the effective batch size is 8 × 1 = 8. Adjust this based on memory and dataset size.
- **Precision:** We enable `fp16=True` (or `bf16=True` if supported) to use half-precision gradients, which is standard when training with BF16 compute on A100.
- **Optimizer:** We use the 8-bit AdamW optimizer provided by bitsandbytes (`optim="paged_adamw_8bit"`), which is memory-efficient and was used in the QLoRA paper ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)).
- **Learning Rate & Scheduling:** A constant learning rate (`lr_scheduler_type="constant"`) with no warmup is often used for LoRA fine-tuning ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)). We choose a learning rate of 1e-4 for a 70B model (larger models often use slightly lower LR, e.g., QLoRA used 1e-4 for 33B/65B ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=33B%20All%2032%201e,4%202343%20512%201024)) and 2e-4 for 7B/13B).
- **Logging & Saving:** Configure `logging_steps` for frequent logging (since dataset is small) and `save_steps` or `save_strategy` to save checkpoints. We also set `max_grad_norm=0.3` for gradient clipping (per QLoRA's recommendations ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))).

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./qlora-70b-cve",       # output directory for checkpoints
    overwrite_output_dir=True,
    num_train_epochs=3,                # small number of epochs for demonstration
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,     # accumulate gradients to simulate batch size 8
    learning_rate=1e-4,
    lr_scheduler_type="constant",      # constant LR (no decay)
    warmup_steps=0,
    optim="paged_adamw_8bit",          # 8-bit AdamW optimizer for efficiency ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer))
    fp16=True,                         # use FP16 mixed precision (or bf16=True if on A100 and desired)
    max_grad_norm=0.3,                 # gradient clipping for stability ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))
    logging_steps=10,
    save_strategy="epoch",
    report_to="none",                  # (or "tensorboard"/"wandb" if logging to a tool)
    gradient_checkpointing=True        # Enable grad checkpointing to reduce memory usage
)
```

> **Note:** We enabled `gradient_checkpointing=True` to trade compute for lower memory usage, which is helpful for large models. With our model already sharded in 4-bit, this further ensures we stay within memory limits. Also, since we are using a single process with `device_map` for multi-GPU, we set `ddp_find_unused_parameters=False` by default (Trainer handles this) because only LoRA params are trainable and all should be used.

## 5. Initializing Trainer and Training on 2 GPUs

Now we create a `Trainer` with our model, dataset, and training arguments. We also need to handle padding in the data collator. The dataset examples may have different lengths, so we'll pad them to the same length per batch. We can define a custom `data_collator` that pads `input_ids`, `attention_mask`, and `labels` to the max length in the batch, using `tokenizer.pad_token_id` for padding and `-100` for label padding (so padded labels don't contribute to loss).

```python
# Define a custom data collator for padding
import numpy as np

def data_collator(batch):
    # Batch is a list of dicts with keys: input_ids, attention_mask, labels
    max_length = max(len(sample["input_ids"]) for sample in batch)
    # Pad sequences to max_length
    input_ids = []
    attention_masks = []
    labels = []
    for sample in batch:
        seq_len = len(sample["input_ids"])
        pad_len = max_length - seq_len
        # Pad input_ids and attention_mask
        input_ids.append(sample["input_ids"] + [tokenizer.pad_token_id] * pad_len)
        attention_masks.append(sample["attention_mask"] + [0] * pad_len)
        # Pad labels with -100
        labels.append(sample["labels"] + [-100] * pad_len)
    # Convert to tensors
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    attention_masks = torch.tensor(attention_masks, dtype=torch.long)
    labels = torch.tensor(labels, dtype=torch.long)
    return {"input_ids": input_ids, "attention_mask": attention_masks, "labels": labels}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,  # use tokenized_dataset (or tokenized_dataset["train"] if a DatasetDict)
    data_collator=data_collator
)
```

Everything is set up. Finally, we launch the training:

```python
trainer.train()
```

During training, you'll see logs every 10 steps (as set by `logging_steps`). The model weights remain in 4-bit on each GPU, and only the LoRA adapter weights are updated. After training, you can save the LoRA adapters (using `model.save_pretrained`) or the full model (which will still consist of the original 70B base in 4-bit plus the small LoRA weights).

**Multi-GPU usage:** Because we used `device_map="auto"` to shard the model, we run this script as a single process (no explicit `accelerate.launch` needed). The Trainer will utilize both GPUs as the model is split across them. If you instead wanted to use distributed data parallel (DDP) with model replication (not usually feasible for 70B on a single GPU), you would omit `device_map` and launch the script with `torchrun` or `accelerate`. In our case, sharding is necessary to fit the model, so one process per node is ideal.

## Summary

We have provided a comprehensive example of fine-tuning a 70B LLaMA 3 model with QLoRA on 2 A100 GPUs. The key steps include loading the model in 4-bit precision ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig)), applying LoRA to all linear layers (r=64, α=16) ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)), preparing prompt-target sequences with proper label masking, and leveraging the Hugging Face Trainer with 8-bit optimizers ([timdettmers/qlora-unnatural-instructions-65b · Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)) and gradient accumulation for efficient multi-GPU training. This script should be ready to run, assuming the dataset and model are correctly specified. Happy fine-tuning!









xxxxx



Got it. I’ll gather the best practices for preparing the cveFixes1 dataset—specifically targeting C code—for fine-tuning a LLaMA 3 model using PEFT/QLoRA. The goal is to create a text-to-text format where the model learns to insert a specific CVE into safe code, transforming it into a vulnerable version.

I’ll also clarify the exact semantics of `function_before` and `function_after` in this dataset to confirm which one is vulnerable and which is fixed.

I’ll be back shortly with the complete guidance.

# Fine-Tuning LLaMA&nbsp;3 on CVE Vulnerability Injection (QLoRA Guide)

## Understanding the CVE Fixes Dataset (`cveFixes1`)

The **cveFixes1** dataset is a collection of code changes associated with CVE fixes across multiple programming languages (C, Go, Java, Python, Ruby). Each entry contains a code snippet **before** and **after** a vulnerability fix, along with metadata like CVE and CWE IDs. In particular, for each C function in the dataset: 

- **`func_before`**: The function **before** the vulnerability was fixed – this is the vulnerable version of the code. In other words, if an entry is labeled as vulnerable, `func_before` contains the code with the CVE vulnerability present.  
- **`func_after`**: The function **after** the vulnerability was fixed – this is the patched (non-vulnerable) version of the code.

For our task, we want the model to learn to **insert a specific CVE vulnerability** into a given piece of safe code. This means our training will use the **fixed code** plus its CVE ID as input, and the **vulnerable code** as the target output. Concretely:
- **Input**: Non-vulnerable function (patched code from `func_after`) + an indicator of the vulnerability (the `cve_id`).  
- **Output**: Vulnerable version of that function (code from `func_before`, which includes the vulnerability).

The dataset’s C subset has about 2,621 examples of such C function pairs. Each example is one function where a CVE-related fix was applied. (Note: Some functions can be quite large, so we may need to handle or filter lengthy examples that exceed the model’s context window.)

## Preparing the Dataset for Text-to-Text Fine-Tuning

To fine-tune LLaMA 3 in a text-to-text fashion, we need to format each example into a prompt and an expected completion. We will create a **prompt** that includes the fixed code and the CVE ID, and a **target** that is the vulnerable code. Here’s a step-by-step plan:

1. **Load the Dataset**: Use Hugging Face’s `datasets` library to load the C subset of cveFixes1. For example:  
   ```python
   from datasets import load_dataset
   data = load_dataset("euisuh15/cveFixes1", split="c")
   ```  
   This gives a dataset where each item has fields like `func_before`, `func_after`, and `cve_id`.

2. **Define the Prompt-Output Format**: We need a clear format so the model can distinguish input from output during training. A simple approach is to concatenate the CVE ID and the fixed code with some separators or prompt text. For example, one might format the training pair as:  
   **Prompt:** *"CVE-{ID}: Below is a C function with the vulnerability fixed. Insert the vulnerability back into this function.\n<code_after_snippet>\nVulnerable version:"*  
   **Target:** *"<code_before_snippet>"* (the vulnerable code).

   In practice, you can make the prompt more straightforward since we have a consistent task. For instance:  
   ```text
   [CVE-2021-1234] 
   Non-vulnerable code: 
   <func_after code here> 
   Vulnerable code:
   ```  
   The model would then be trained to produce the vulnerable code after the "Vulnerable code:" prompt.

3. **Construct Prompt-Target Pairs**: Using the loaded dataset, iterate through each example and build the prompt and target strings. In code, this might look like:  
   ```python
   def make_prompt_and_target(example):
       cve = example["cve_id"]
       fixed_code = example["func_after"]
       vuln_code = example["func_before"]
       prompt = f"CVE-{cve}:\nFixed function:\n{fixed_code}\n\nVulnerable version:\n"
       target = vuln_code
       return {"prompt": prompt, "target": target}
   
   data = data.map(make_prompt_and_target, remove_columns=data.column_names)
   ```  
   This will produce a new dataset with just `"prompt"` and `"target"` fields for each sample, where `"prompt"` is the input text and `"target"` is the expected output text.

4. **Verify Example Formatting**: It’s good to double-check one example to ensure the format is correct. For instance:  
   **Prompt example:**  
   ```
   CVE-2014-0160:
   Fixed function:
   static int heartbeat() {
       // ... (safe code)
   }
   
   Vulnerable version:
   ```  
   **Target example:** (the model should output)  
   ```
   static int heartbeat() {
       // ... (code with the Heartbleed vulnerability)
   }
   ```  
   Make sure the prompt clearly separates the fixed code from where the vulnerable code should start. The newline and "Vulnerable version:" (or any chosen marker) before the target help indicate where the model’s output begins.

## Choosing a Data Format (JSONL vs. HuggingFace Dataset)

For fine-tuning with Hugging Face Transformers and PEFT, using a **Hugging Face Dataset** object is very convenient. The `datasets` library can handle large datasets efficiently and works well with the Trainer API. Two common approaches are:

- **Hugging Face `Dataset` Object**: After creating the prompt-target pairs as shown above, you can keep the data in a `Dataset` and feed it to the training pipeline. This is convenient for directly using the Hugging Face Trainer, which can take a `Dataset` for training. You may further split into train/val if needed (for example, `data.train_test_split`).

- **JSONL (JSON Lines) File**: This is a simple text file format where each line is a JSON object. You could export the dataset to JSONL if you plan to use custom loading or need to share the processed data. For example, Hugging Face Datasets allows saving to disk:  
  ```python
  data.to_json("finetune_cve_injection.jsonl", orient="records", lines=True)
  ```  
  This would produce a file where each line has `{"prompt": "...","target": "..."}`. JSONL is human-inspectable and easily loaded if you prefer writing a custom DataLoader.

In practice, if you intend to use the Hugging Face Trainer/PEFT pipeline, you can keep the data in memory as a `Dataset`. If you prefer a custom PyTorch training loop, you might convert it to a PyTorch `DataLoader`. Either approach is fine; what matters is that the format cleanly separates input and output text. 

**Recommendation**: Use the Hugging Face Dataset format for integration with Transformers. It natively supports methods to tokenize and collate data. You can always export to JSONL for backup or debugging, but it’s not strictly required for fine-tuning.

## Preprocessing and Tokenization for LLaMA 3

Before training, we need to tokenize our prompt and target texts using LLaMA 3’s tokenizer. LLaMA 3 (like LLaMA 2) is a decoder-only model, meaning it expects a single sequence of tokens as input, and we will train it in a causal language modeling manner. Key considerations:

- **Load the Tokenizer**: Use the appropriate tokenizer for the LLaMA 3 model (from Hugging Face, e.g., `AutoTokenizer`). For example:  
  ```python
  from transformers import AutoTokenizer
  tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-7b-hf")  # hypothetical name
  tokenizer.pad_token = tokenizer.eos_token  # LLaMA uses EOS as padding if needed
  ```  
  Ensure you have the correct model checkpoint name for LLaMA 3.

- **Tokenize Prompt and Target**: We will combine the prompt and target into one sequence for the model, but we need to keep track of which tokens are the prompt (so we can mask them out in the loss). A common strategy is:  
  1. Tokenize the **prompt** and the **target** separately.  
  2. Concatenate them, and create a label array that is `-100` (ignore index) for all prompt tokens and equals the token IDs for the target part ([Large Language Model Finetuning Practice | by Haifeng Zhao](https://medium.com/@piscaries/large-language-model-finetuning-practice-7e131291046e#:~:text=,return)). This way, during training the loss is only computed for the target (vulnerable code) tokens, not for regurgitating the prompt. 

  For example:  
  ```python
  def tokenize_func(example):
      prompt_ids = tokenizer.encode(example["prompt"], add_special_tokens=False)
      target_ids = tokenizer.encode(example["target"], add_special_tokens=False)
      # Ensure an EOS token between or at end:
      input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]
      # Create labels: -100 for prompt, and actual ids for target (plus EOS)
      labels = [-100] * len(prompt_ids) + target_ids + [tokenizer.eos_token_id]
      return {"input_ids": input_ids, "labels": labels}
  
  tokenized_data = data.map(tokenize_func, remove_columns=["prompt","target"])
  ```  
  In this scheme, `input_ids` contains the whole sequence (prompt followed by target), and `labels` has `-100` for each prompt token position (so the model isn’t trained to output the prompt) and the actual token IDs for the vulnerable code portion ([Large Language Model Finetuning Practice | by Haifeng Zhao](https://medium.com/@piscaries/large-language-model-finetuning-practice-7e131291046e#:~:text=,return)).

- **Handle Length and Truncation**: LLaMA 3 models typically have a context length (e.g., 2048 or 4096 tokens). Some functions in cveFixes1 are very large, possibly exceeding this. You should decide on a maximum sequence length (perhaps the model’s max context or a bit less to allow for the prompt + output). In the tokenization step, use `tokenizer(truncation=True, max_length=N)` for a safe `N` (like 1024 or 2048, depending on GPU memory and model context). Truncate from the end of the sequence if needed (or from the code – but generally you want to ensure the beginning of the prompt is intact, which it will be if truncating at end, possibly cutting off some of the target if it’s too long).

- **Padding**: If you use a `DataCollator` to batch examples, ensure it handles padding. For example, `DataCollatorForLanguageModeling` with `mlm=False` will pad sequences to the same length in a batch and set padding token labels to `-100` by default. Since we already set `-100` for prompt tokens, any additional padding should also be `-100` (the collator typically does this). Setting `tokenizer.pad_token = tokenizer.eos_token` (as above) helps avoid errors with models that have no pad token.

- **Validation**: It’s wise to double-check that for a sample, the `input_ids` correspond to `[Prompt tokens] [Target tokens] <eos>` and `labels` are `[-100,...-100, token_ids_of_target..., eos_id]`. This ensures the model will learn to predict only the vulnerable code given the fixed code context.

## Setting Up LLaMA 3 with QLoRA (via PEFT)

With our data ready, we can now configure the model for efficient fine-tuning. We’ll use **QLoRA**, which means we load LLaMA 3 in 4-bit precision and attach LoRA adapters (low-rank weight updates) to train on our dataset. The Hugging Face [PEFT](https://github.com/huggingface/peft) library makes this straightforward. Here are the steps:

1. **Load the Base Model in 4-bit**: Use `bitsandbytes` integration to load the model in 4-bit precision. We create a `BitsAndBytesConfig` for 4-bit NormalFloat (NF4) quantization. For example:  
   ```python
   from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   bnb_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_quant_type="nf4",
       bnb_4bit_use_double_quant=True,
       bnb_4bit_compute_dtype=torch.float16  # or torch.bfloat16 if using newer GPUs
   )
   model = AutoModelForCausalLM.from_pretrained(
       "meta-llama/Llama-3-7b-hf", 
       quantization_config=bnb_config,
       device_map="auto"  # if using multiple GPUs or "cpu"/"cuda" as needed
   )
   ```  
   This loads the LLaMA 3 model weights in 4-bit precision. The `nf4` quantization type and double quantization are recommended for QLoRA. 

2. **Prepare Model for QLoRA Training**: There’s a utility in PEFT to prepare a quantized model for training. This will tweak some model settings (like turning off weight decay for certain layers, etc.). Use:  
   ```python
   from peft import prepare_model_for_kbit_training
   model = prepare_model_for_kbit_training(model)
   ```  
   This step handles some low-level fixes (for example, making sure gradient checkpointing is enabled and some layers are cast to FP32 as needed). Now the model is ready for attaching LoRA adapters.

3. **Configure LoRA**: Decide on LoRA hyperparameters. Key parameters are:
   - `r`: LoRA rank (the dimensionality of the update matrices). Higher `r` means more capacity to learn (but also more memory use). Common values are 8, 16, 32, or 64. For an 8–13B model, `r=16` or `32` is often used; for very complex tasks or larger models, `r=64` can be beneficial at the cost of more VRAM. You can start with `r=16` and increase if the model underfits.
   - `lora_alpha`: Scaling factor for the LoRA updates. Often set proportional to `r` (like alpha = r or 2*r). For example, many LoRA setups use `alpha = 2*r` or `alpha = r`. In one LLaMA-3 8B example, `r=64` and `alpha=16` was used (so alpha was smaller in that case). The QLoRA paper often used α=16 or 32 for various setups. You can set `lora_alpha=16` as a reasonable default and adjust if needed.
   - `lora_dropout`: Dropout applied to LoRA layers. If your dataset is small (a few thousand examples), a bit of dropout (e.g. 0.05–0.1) helps prevent overfitting. If you have plenty of data or want maximum retention of details, you can use 0.0.
   - `target_modules`: Which layers to apply LoRA to. For LLaMA models, the safe bet is to target all the key weight matrices in the Transformer blocks (e.g. the query, key, value, and output projection of self-attention). In LLaMA these are named `"q_proj"`, `"k_proj"`, `"v_proj"`, `"o_proj"`. Some setups also target the feed-forward layers (e.g., `"gate_proj"`, `"down_proj"`, `"up_proj"` for LLaMA-2 architecture) by using `target_modules="all-linear"` which applies LoRA to *every* linear layer. Targeting all linear layers can improve fine-tuning efficacy at the cost of more parameters. For initial experiments, you can stick with attention projections only, and later consider expanding if needed.
   - `bias`: Usually `"none"` (we don’t train any bias terms in LoRA).

   Now create the LoRA config and wrap the model:  
   ```python
   from peft import LoraConfig, get_peft_model
   lora_config = LoraConfig(
       r=16,
       lora_alpha=16,
       target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # or "all-linear"
       lora_dropout=0.05,
       bias="none",
       task_type="CAUSAL_LM"
   )
   model = get_peft_model(model, lora_config)
   model.print_trainable_parameters()
   ```  
   This will transform the model so that it has trainable LoRA layers (and all original weights are frozen in 4-bit). The `print_trainable_parameters()` should report a small fraction of total parameters as trainable. For example, with `r=16` on a 7B model, trainable params might be on the order of tens of millions (<<1% of total).

## Training Configuration and Execution (QLoRA Fine-Tuning)

With model and data ready, the final step is to set up training. Key aspects include the training loop or Trainer, learning rate, batching, and number of epochs:

- **Training Loop**: You can use the Hugging Face `Trainer` API to simplify training. It will handle feeding data to the model and applying optimizers. For QLoRA, it's recommended to use a **paged AdamW optimizer** from bitsandbytes for efficiency ([QLoRA · GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,enable%20for)). Fortunately, Transformers v4.30+ allows specifying `optim="paged_adamw_8bit"` in `TrainingArguments` to use the 8-bit optimizer which is memory-efficient for LoRA ([QLoRA · GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,enable%20for)). 

  Set up `TrainingArguments` with appropriate values:  
  ```python
  from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

  training_args = TrainingArguments(
      output_dir="./llama3-cve-qlora",
      per_device_train_batch_size=1,  # adjust based on GPU memory
      gradient_accumulation_steps=8,  # accumulate grads for larger effective batch
      num_train_epochs=3,
      learning_rate=2e-4,
      warmup_steps=100,
      logging_steps=50,
      optim="paged_adamw_8bit",
      fp16=True,  # use mixed precision if supported
      evaluation_strategy="no",  # or "steps"/"epoch" if you have a eval set
      save_strategy="epoch",
      report_to="none"
  )

  # Data collator to pad sequences and mask out prompt portion appropriately
  data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_data,
      data_collator=data_collator
  )
  trainer.train()
  ```  
  In the above: 
  - `per_device_train_batch_size` is set to 1 as a safe default given the potentially long sequences; you can increase it if memory allows.
  - `gradient_accumulation_steps=8` effectively means a batch of 8 sequences before an optimizer step (simulate batch_size = 8). Adjust this along with the per-device batch to achieve a desired effective batch size.
  - `learning_rate` for LoRA is often in the range 2e-4 to 3e-4 ([QLoRA · GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,%29%2C%5Cn)). We use 2e-4 with a small warmup. Monitor loss and adjust if needed (if underfitting, could increase; if unstable, decrease).
  - `fp16=True` to use half-precision compute for speed (since the base model is 4-bit, this mainly affects optimizer states and LoRA computations).
  - We turned off actual evaluation (`evaluation_strategy="no"`) for simplicity, but if you have a validation split, you can set it to evaluate every X steps or each epoch.
  - We use the `DataCollatorForLanguageModeling` with `mlm=False`, which will ensure that the `labels` tensor is correctly created. Since we already set up `labels` in the dataset with `-100` for prompt tokens, the collator should leave those as is. It will pad `input_ids` and `labels` to the longest sequence in each batch, padding labels with `-100` as needed (so padding tokens are ignored in loss).

- **Training Process**: As training runs, the model will learn to generate the vulnerable code given the fixed code and CVE context. Watch the training loss to ensure it's decreasing. Given the dataset size (~2.6k examples for C), a few epochs (2–4) are usually enough. Too many epochs might cause overfitting (the model might memorize patterns and not generalize).

- **PEFT Saving**: By default, Trainer will save the full model (with LoRA adapters merged into a PeftModel). You can also choose to save just the LoRA adapter weights (using `model.save_pretrained` from PEFT). If using the Trainer’s default save, it will save a minimal checkpoint that includes the base model reference and the LoRA weights – which is fine. After training, you can reload the model for inference via:  
  ```python
  from peft import PeftModel, PeftConfig
  base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-7b-hf", device_map="auto", quantization_config=bnb_config)
  model = PeftModel.from_pretrained(base_model, "./llama3-cve-qlora")
  model.eval()
  ```
  This loads the base 4-bit model and then applies the trained LoRA weights.

- **Inference Check**: To test, feed a prompt in the same format as training (fixed code + CVE) and see if the model generates a plausible vulnerable version. Make sure to stop generation at an appropriate token (you might rely 










