Thanks for the image â€” the traceback shows this error in your `format_example` function:

```
TypeError: string indices must be integers, not 'str'
```

### Root Cause:
This means you're trying to index into a **string** as if it were a **dictionary**. Specifically, the error is at this line:
```python
return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
        f"{example['func_after']}\n\n### Response:\n{example['func_before']}")
```

The issue is that `example` is actually a **string**, not a dictionary â€” likely because you're passing a **batched string** to the function instead of an object like `{"cve_id": ..., "func_after": ..., "func_before": ...}`.

---

### Fix:

Update your `formatting_prompts_func` like this:

```python
def formatting_prompts_func(example):
    return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
            f"{example['func_after']}\n\n### Response:\n{example['func_before']}")
```

Then **remove the batching**. Pass this function as `formatting_func`, not wrapped in a list comprehension.

Also, make sure you're not wrapping it again in a batch:

```python
# No list comprehension needed
trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    data_collator=collator,
    formatting_func=formatting_prompts_func,  # NOT a batched version
    args=sft_config
)
```

If the `SFTTrainer` expects a batched version (which depends on your TRL version), you can instead add a check:

```python
def formatting_prompts_func(example_or_batch):
    if isinstance(example_or_batch, dict) and 'cve_id' in example_or_batch:
        return (f"Insert vulnerability CVE-ID {example_or_batch['cve_id']} into the following function:\n"
                f"{example_or_batch['func_after']}\n\n### Response:\n{example_or_batch['func_before']}")
    elif isinstance(example_or_batch, list):
        return [formatting_prompts_func(ex) for ex in example_or_batch]
    else:
        raise ValueError("Invalid input format for formatting function")
```

This version is robust and handles both single examples and batches.

Let me know if you want help testing it live or checking token lengths per example.






# Now that tokenizer is available, create the data collator for completion-only language modeling
collator = DataCollatorForCompletionOnlyLM(
    response_template="### Response:",
    tokenizer=tokenizer,
    mlm=False  # not doing masked language modeling, this is causal LM
)

# Define training arguments via SFTConfig (which is similar to HF TrainingArguments)
sft_config = SFTConfig(
    model_name_or_path=None,  # not needed since we pass the model object to trainer
    output_dir="./llama3-70b-vuln-qlora", 
    num_train_epochs=3,
    learning_rate=2e-4,
    lr_scheduler_type="cosine",          # cosine decay schedule (just an example choice)
    warmup_steps=100,                    # brief warmup for stability
    per_device_train_batch_size=2,
    gradient_accumulation_steps=8,       # accumulate gradients to simulate larger batch
    optim="paged_adamw_8bit",            # use memory-efficient 8-bit AdamW optimizer&#8203;:contentReference[oaicite:26]{index=26}
    logging_steps=50,
    evaluation_strategy="epoch",
    save_strategy="epoch",
    bf16=True,        # use bfloat16 precision for training (if supported by your GPU, else use fp16)
    tf32=True         # enable TF32 on Ampere GPUs for faster math (optional)
)

# Initialize the SFT Trainer
trainer = SFTTrainer(
    model=model, 
    train_dataset=train_dataset, 
    eval_dataset=eval_dataset,
    data_collator=collator,
    formatting_func=formatting_prompts_func,  # apply our prompt formatting
    args=sft_config
)

# Train the model!
trainer.train()




# Evaluate on validation set
eval_results = trainer.evaluate()
val_loss = eval_results["eval_loss"]
print(f"Validation loss: {val_loss:.4f}")
try:
    import math
    print(f"Validation perplexity: {math.exp(val_loss):.2f}")
except OverflowError:
    print("Validation perplexity is inf (loss might be too high or not applicable).")







xxxxx


Thanks for the details. Iâ€™ll prepare the data processing pipeline and fine-tuning code for using QLoRA/PEFT to train a LLaMA 3 70B Instruct model where the model learns to inject specific CVEs into code. I'll structure the dataset so that:

- **Input**: Instruction to insert a given CVE ID into the code, followed by the non-vulnerable code (`function_after`).
- **Output**: Vulnerable version of the code (`function_before`).

I'll include:
- Code to load and preprocess the dataset
- Format each sample into instruction-tuning format compatible with LLaMA 3
- Split into train and validation (90/10)
- Fine-tuning code using PEFT/QLoRA

Iâ€™ll let you know once everything is ready.

# Fine-tuning LLaMA 3 70B with QLoRA for Vulnerability Injection

## Introduction and Overview

In this guide, we will fine-tune Meta's **LLaMA 3 70B Instruct** model (70 billion parameters) to perform a specialized code transformation task: **inserting vulnerabilities** (based on specific CVE IDs) into given code functions. The training data comes from the *CVEfixes1* dataset, which contains thousands of real-world vulnerability-fixing code changes (before-and-after code for over 5,000 CVEs) ([CVEfixes Dataset: Automatically Collected Vulnerabilities and Their Fixes from Open-Source Software](https://zenodo.org/records/4476564#:~:text=organized%20as%20a%20relational%20database,18249%20files%2C%20and%2050322%20functions)). We will use **instruction tuning**: each training sample is a prompt instructing the model to inject a particular CVE vulnerability into a piece of code, and the expected output is the code with that vulnerability injected.

Because a 70B model is extremely large, we employ **QLoRA (Quantized Low-Rank Adaptation)** ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)) in combination with Hugging Face's **PEFT (Parameter-Efficient Fine-Tuning)** framework. QLoRA allows fine-tuning a 70B model on a single or few GPUs by: (a) loading the base model in 4-bit precision (to drastically reduce memory), and (b) attaching small Low-Rank Adapter (LoRA) weights which are the only trainable parameters ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,innovations%20to%20save%20memory%20without)). The base model remains frozen (in 4-bit quantized form) and gradients update only the LoRA layers. This approach preserves nearly full fine-tuning performance while using a fraction of the memory of full 16-bit training ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=,new%20data%20type%20that%20is)). We will also enable **gradient checkpointing** (to save memory by recomputing activations), and take advantage of **FlashAttention** for faster training if available ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=pip%20install%20)). Model parallelism (distributing the model across GPUs) will be handled automatically by Hugging Face Accelerate's `device_map`, since a 70B model in 4-bit may still need multiple devices.

**Key steps in this tutorial:**

- **Data Preparation:** Load the CVE fixes dataset, combine code snippets, and format each example as an instruction-response pair (with a 90/10 train/validation split).
- **Model Loading (LLaMA 3 70B Instruct):** Load the LLaMA 3 70B instruct model and tokenizer in 4-bit precision using `bitsandbytes`, verifying we use LLaMA 3's new tokenizer (with expanded vocabulary) ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the)) and 8k token context window ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=The%20Llama%203%20release%20introduces,context%20length%20of%208K%20tokens)).
- **QLoRA Setup:** Prepare the model for 4-bit training (casting certain layers to float32 for stability) and add LoRA adapters to the appropriate layers (e.g. query/key projection layers of the Transformer).
- **Training Configuration:** Define hyperparameters and a training loop using Hugging Face Transformers + TRL. We'll use the `SFTTrainer` from ðŸ¤— TRL (Transformer Reinforcement Learning library) for supervised fine-tuning, along with a special data collator that ensures only the *response* portion of each sample is used for computing the loss (so the model is trained to predict the vulnerable code given the prompt).
- **Optimization for 70B:** Enable gradient checkpointing, use a memory-efficient optimizer (8-bit AdamW ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))ã€‘, and optionally FlashAttention 2 for faster attention computatio ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=pip%20install%20))ã€‘.
- **Running Training & Evaluation:** Train the model on the training set and evaluate on the held-out validation set. We will demonstrate how to save the LoRA adapter and how to use the fine-tuned model to generate vulnerable code given a new instruction.

Throughout this guide, we include well-documented code snippets and explanations for each step.

## 1. Setup and Dataset Preparation

First, install and import the required libraries: Hugging Face ðŸ¤— Transformers, Datasets, PEFT, TRL, and BitsAndBytes (for 4-bit quantization). Ensure you have a compatible GPU with sufficient memory (multiple GPUs may be needed for a 70B model). Then load the **cveFixes1** dataset from Hugging Face. This dataset contains code **before** and **after** fixing a vulnerability, along with the CVE ID and other metadata. We will combine all programming languages in the dataset and then split into 90% train and 10% validation.

```python
!pip install transformers accelerate peft trl bitsandbytes datasets

from datasets import load_dataset

# Load the CVEfixes1 dataset. It has splits by language; we combine them.
dataset = load_dataset("euisuh15/cveFixes1")  # returns a dict of splits: 'c', 'go', 'java', 'python', 'ruby'

# Concatenate all language splits into one dataset
all_splits = [dataset[lang] for lang in dataset.keys()]  # dataset.keys() -> ['c','go','java','python','ruby']
full_dataset = all_splits[0].concatenate(*all_splits[1:])

print(f"Total examples (all languages): {len(full_dataset)}")  # Expect ~4380 examples in total

# Shuffle and split 90/10 into train and validation sets
full_dataset = full_dataset.shuffle(seed=42)
train_val = full_dataset.train_test_split(test_size=0.1, seed=42)
train_dataset = train_val["train"]
eval_dataset = train_val["test"]

print(f"Train examples: {len(train_dataset)}; Validation examples: {len(eval_dataset)}")
```

**Explanation:** The dataset provides code in several languages (C, Java, Python, etc.), each containing functions *before* and *after* a vulnerability fix. By combining them, we get around 4.3K function pairs. We then shuffle and split the data into a training set (â‰ˆ90%) and a validation set (â‰ˆ10%). This ensures the model is evaluated on code it hasn't seen during training.

Each dataset record has fields like `func_before` (the vulnerable function), `func_after` (the fixed function), and `cve_id` (the identifier of the vulnerability). For our fine-tuning, we only need these three fields:
- **`cve_id`**: e.g., "CVE-2014-0160"
- **`func_after`**: the function *after* the fix (i.e., secure code).
- **`func_before`**: the function *before* the fix (i.e., vulnerable code).

## 2. Formatting Data for Instruction Tuning

We will format each training example as an **instruction** and an **expected response**. The instruction will prompt the model to insert a specific vulnerability into a given function. According to the specification:

- **Prompt (Instruction)**: *"Insert vulnerability CVE-ID `{cve_id}` into the following function:"* followed by the **`func_after`** code (the non-vulnerable code).
- **Expected Output (Response)**: The corresponding **`func_before`** code (the vulnerable version of the function).

To clearly separate the instruction from the model's response during training, we will insert a special marker **"### Response:"** before the vulnerable code. This marker will help our training collator distinguish the prompt from the completion. (This follows a common format in instruction tuning where prompts and responses are separated by tokens like `"### Response: ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=For%20instruction%20fine,This%20allows%20people%20to))1ã€‘.) For example, a formatted sample might look like:

```
Insert vulnerability CVE-ID CVE-2014-0160 into the following function:
<secure function code here>

### Response:
<vulnerable function code here>
```

We now prepare a formatting function and a data collator to apply this template to our dataset during training:

```python
from trl import SFTTrainer, SFTConfig, DataCollatorForCompletionOnlyLM

# Define a formatting function to combine fields into a single prompt-response string
def format_example(example):
    return (f"Insert vulnerability CVE-ID {example['cve_id']} into the following function:\n"
            f"{example['func_after']}\n\n### Response:\n{example['func_before']}")

# Wrap the formatting function to work on lists of examples for efficiency (vectorized mapping)
def formatting_prompts_func(batch):
    return [format_example(ex) for ex in batch]

# Initialize a data collator that will mask out the prompt portion in the loss.
tokenizer = None  # We will load the tokenizer later after model is loaded.
# We'll create the collator after initializing the tokenizer and model, because it needs the tokenizer.
```

**Explanation:** The `format_example` function constructs the instruction and response text. We include two newline characters before `"### Response:"` to ensure the response starts on a new line for clarity. During fine-tuning, the model will see the full text up to `"### Response:"` as the input prompt, and it should learn to produce the text *after* `"### Response:"` (the vulnerable code). We will use Hugging Face TRL's `DataCollatorForCompletionOnlyLM` to handle tokenization and labeling. This collator will tokenize the combined text, find the `"### Response:"` marker, and set the **labels** such that all tokens in the prompt (everything before and including the `"### Response:"` marker) are **ignored (label -100)**, and only the tokens of the response (the vulnerable code) are **predicted by the model ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=For%20instruction%20fine,This%20allows%20people%20to)) ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=You%20can%20use%20the%20,how%20it%20would%20work%20to))9ã€‘. This way, the loss is computed only on the model's output portion.

We haven't loaded the tokenizer or model yet, so we left the tokenizer as `None` for now. In the next step, we'll load the LLaMA 3 model and tokenizer, then finalize the collator.

## 3. Loading LLaMA 3 70B Instruct Model with 4-bit Quantization (QLoRA)

Now let's load the pre-trained **LLaMA 3 70B Instruct** model from Hugging Face. We use the instruct variant of LLaMA 3 70B (which has already been fine-tuned by Meta for following instruction ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=The%20Llama%203%20release%20introduces,context%20length%20of%208K%20tokens))8ã€‘ as our base model. It's important to use the correct **tokenizer** for LLaMA 3, since LLaMA 3 uses a new tokenizer with a much larger vocabulary (128k tokens vs 32k in LLaMA  ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the))4ã€‘. This tokenizer is needed to properly encode code and text for the model.

Because the model is so large, we'll load it in 4-bit precision using **BitsAndBytes** (via `quantization_config`). We also disable `use_cache` and enable gradient checkpointing to reduce memory usage during training. Gradient checkpointing will force the model to recompute certain layers on the fly instead of storing all intermediate activations, trading compute for memory â€“ a necessary trade-off for 70B models.

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import prepare_model_for_kbit_training

model_name = "meta-llama/Meta-Llama-3-70B-Instruct"  # placeholder HF model name for LLaMA3 70B instruct

# Define 4-bit quantization configuration (using NF4 quantization, per QLoRA pap ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4ã€‘)
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_use_double_quant=True,   # double quantization to reduce memo ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4ã€‘
    bnb_4bit_quant_type="nf4",        # NormalFloat4 (NF4) quantizati ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4ã€‘
    bnb_4bit_compute_dtype=torch.bfloat16  # use bfloat16 for computation (or float16 if bfloat16 not available)
)

# Load the tokenizer
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
# Note: use_fast=False because LLaMA tokenizer is usually implemented as a slow tokenizer. 
# Ensure the tokenizer is the one for LLaMA3 to get the 128k voc ([Welcome Llama 3 - Meta's new open LLM](https://huggingface.co/blog/llama3#:~:text=A%20big%20change%20in%20Llama,the%208B%20version%20of%20the))4ã€‘.

# Load the model in 4-bit mode across available devices
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",    # automatically split layers across GPUs (or CPU if needed)
    use_cache=False,      # disable cache for gradient checkpointing
    trust_remote_code=True  # if the model repo requires custom code (may not be needed for LLaMA3)
)

# Prepare model for 4-bit training
model.gradient_checkpointing_enable()  # enable gradient checkpointing for memory savings
model = prepare_model_for_kbit_training(model)
```

**Explanation:** We use `AutoTokenizer` and `AutoModelForCausalLM` to load the tokenizer and model. The `BitsAndBytesConfig` sets the model to load in 4-bit quantized mode (with NF4 quantization and double quantizatio ([[2305.14314] QLoRA: Efficient Finetuning of Quantized LLMs](https://arxiv.org/abs/2305.14314#:~:text=sacrificing%20performance%3A%20%28a%29%204,performance%20across%208%20instruction%20datasets))4ã€‘. The `device_map="auto"` will automatically distribute the model's layers across all available GPUs (and CPU memory if necessary) using Hugging Face Accelerate. This is a form of model parallelism that is essential for handling a 70B model. For example, if you have 2Ã—40GB GPUs, it might put roughly half the model on each GPU. If you only have a single GPU with enough memory (e.g., 48GB), it will put the whole model there. If the model still doesn't fit, some layers could be sharded to CPU RAM (which slows down training). You can adjust the `device_map` or use more advanced parallelism (like Fully Sharded Data Parallel) if needed, but `"auto"` is a convenient starting point.

We set `use_cache=False` because the modelâ€™s key/value cache is not needed during training and disabling it is required when using gradient checkpointing to avoid inconsistencies. We explicitly enable gradient checkpointing via `model.gradient_checkpointing_enable()`, and call `prepare_model_for_kbit_training(model)`. This utility from PEFT adjusts the model for low-bit training: it casts certain layers (e.g., layer norms and output embeddings) to FP32 for stability, and may handle some device placement tweaks for int8/4-bit traini ([Optimizing Language Model Fine-Tuning with PEFT, QLORA ...](https://medium.com/@tejpal.abhyuday/optimizing-language-model-fine-tuning-with-peft-qlora-integration-and-training-time-reduction-04df39dca72b#:~:text=Optimizing%20Language%20Model%20Fine,initializes%20the%20model%20for))7ã€‘. At this point, the 70B model is loaded in memory in 4-bit form, but we have not added our LoRA adapters yet.

**Optional â€“ Flash Attention:** If you have the [FlashAttention 2](https://github.com/HazyResearch/flash-attention) library installed, you can **speed up attention computation** and reduce memory usage further. Simply install the library (`pip install flash-attn`) and when loading the model, pass `attn_implementation="flash_attention_2"` to `from_pretraine ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=To%20use%20Flash%20Attention%202%2C,attn%60%20package))9ã€‘. This uses optimized CUDA kernels for self-attention (supported on NVIDIA GPUs with SM>=70, e.g., A100, V100). FlashAttention 2 can be used with quantization and checkpointing for additional efficien ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=Note%20that%20Flash%20Attention%20only,other%20tools%20such%20as%20quantization)) ([Supervised Fine-tuning Trainer](https://huggingface.co/docs/trl/en/sft_trainer#:~:text=In%20contrast%20to%20Flash%20Attention,that%20also%20includes%20padding%20tokens))0ã€‘. If not installed, the model will use the default PyTorch implementation (which on PyTorch 2.x may already leverage FlashAttention under the hood for some operations).

## 4. Applying LoRA Adapters with PEFT

With the base model loaded and prepared, the next step is to configure LoRA adapters. LoRA will inject trainable weight matrices into the model's attention (and/or feed-forward) layers. Given the standard practice and QLoRA's recommendations, we will target the **query (`q_proj`) and value (`v_proj`) projection matrices** in each Transformer attention block for LLaMA. This keeps the number of trainable parameters small while still being effective at modeling new behavior (since Q and V are crucial for attention dynamics).

We choose a LoRA **rank** of, say, 8 or 16 (this controls the dimensionality of the low-rank updates; higher rank means more capacity but more VRAM usage). We also set a **LoRA scaling (alpha)**, often set to 16 or 32, which is a factor that scales the LoRA updates. We will not train any new biases (setting `bias="none"`). These hyperparameters can be tuned; here we use rank=8 and alpha=16 for demonstration, which yields a very lightweight adapter.

Let's attach LoRA to the model:

```python
from peft import LoraConfig, get_peft_model

# Configure LoRA
lora_config = LoraConfig(
    r=8,             # LoRA rank (increased for more capacity if needed, e.g., 16 or 32)
    lora_alpha=16,   # LoRA scaling factor
    target_modules=["q_proj", "v_proj"],  # which sub-modules to apply LoRA to (LLaMA's attention projections)
    bias="none", 
    task_type="CAUSAL_LM"  # type of task
)

# Wrap the model with PEFT LoRA
model = get_peft_model(model, lora_config)

# Print the number of trainable parameters to verify we are tuning a small subset
trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
total_params = sum(p.numel() for p in model.parameters())
print(f"Trainable params: {trainable_params} (~{100 * trainable_params/total_params:.4f}% of total)")
```

**Explanation:** We create a `LoraConfig` specifying how to inject the LoRA layers. By targeting `"q_proj"` and `"v_proj"` in each transformer block, we








xxxxxxx

Thanks! Since you're using 2 A100 GPUs and want to fine-tune the LLaMA 3 70B Instruct model with QLoRA via PEFT, Iâ€™ll put together updated training code that:

- Loads the 70B model in 4-bit with BitsAndBytes
- Uses Hugging Face PEFT to add LoRA adapters
- Sets up `TrainingArguments` with `gradient_accumulation`, `fp16`, and multi-GPU support
- Handles tokenizer, data collation, and masking properly for causal language modeling

Iâ€™ll include recommendations for LoRA config tuned for 70B (e.g., `r=64`, `target_modules='all-linear'`, `lora_dropout=0.1`) and how to train with Hugging Face Trainer.

Iâ€™ll get that code and guidance ready for you shortly.

# QLoRA Fine-tuning Example for LLaMA 3 70B Instruct

In this example, we show how to fine-tune a **LLaMA-3 70B Instruct** model on a small CVE vulnerability injection dataset using **QLoRA** (Quantized LoRA) with Hugging Face Transformers and PEFT. The code below loads the 70B model in 4-bit precision, adds LoRA adapters to all linear layers, prepares the dataset of prompt-target pairs, and runs efficient multi-GPU training on 2Ã—A100 GPUs.

## 1. Setup and Model Loading (4-bit Quantization)

First, install the required libraries if you haven't already (Transformers, PEFT, BitsAndBytes, Accelerate):

```bash
pip install transformers accelerate bitsandbytes peft
```

Now we load the 70B model in **4-bit precision** using `bitsandbytes`. We use `device_map="auto"` to automatically shard the model across the two GPUs, and configure 4-bit **NF4** quantization with double quantization for efficiency ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig)). We also load the corresponding tokenizer and set the padding token:

```python
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig

model_name = "<path-or-name-of-llama3-70b-instruct>"
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type="nf4",         # NormalFloat4 quantization ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig))
    bnb_4bit_use_double_quant=True,    # Use double quantization for memory save ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig))
    bnb_4bit_compute_dtype=torch.bfloat16  # Use BF16 for computation on A100 ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))
)

# Load the tokenizer and model in 4-bit, sharding across GPUs
tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=False)
if tokenizer.pad_token_id is None:
    tokenizer.pad_token_id = tokenizer.eos_token_id  # Set pad token to EOS (common practice for LLaMA)
model = AutoModelForCausalLM.from_pretrained(
    model_name,
    quantization_config=bnb_config,
    device_map="auto",    # Automatically distribute layers across available GPUs ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20AutoModelForCausalLM))
    use_auth_token=True   # if model is behind authentication (optional)
)
```

> **Note:** Loading with `device_map="auto"` will place different layers of the 70B model on each GPU, enabling one process to utilize both GPUs as shared memory for the model ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20AutoModelForCausalLM)). This means you do **not** need to launch multiple processes for distributed training in this setup. The model remains in 4-bit quantized mode (NF4) but computations use higher precision (BF16) for accuracy ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)).

## 2. Configuring LoRA with PEFT

Next, we configure the LoRA parameters using the PEFT library. We apply LoRA to **all linear layers** of the model (Q, K, V, O in self-attention and the gated feed-forward projections) as recommended by QLoRA ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)). We'll use LoRA rank `r=64` and set `lora_alpha=16` (scaling factor) following the QLoRA paper settings ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)). The LoRA dropout is set to `0.1` (10%), which is a typical value for models up to 13B ([LoRA Fine-tuning & Hyperparameters Explained (in Plain English) | Entry Point AI](https://www.entrypointai.com/blog/lora-fine-tuning/#:~:text=The%20QLoRA%20paper%20set%20dropout,for%2033B%20and%2065B%20models)) (for 70B, the QLoRA paper suggests 0.05, but we use 0.1 as an example). We target all transformer linear layer names (`q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj`) for LoRA injection:

```python
from peft import LoraConfig, get_peft_model

lora_config = LoraConfig(
    r=64,
    lora_alpha=16,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    lora_dropout=0.1,
    bias="none",           # We will not tune biases
    task_type="CAUSAL_LM"  # Language model fine-tuning
)
# Wrap the base model with LoRA adapters
model = get_peft_model(model, lora_config)
model.print_trainable_parameters()  # Optional: verify number of trainable params vs total
```

This will add trainable LoRA weight matrices to each target linear layer while keeping the original 70B weights frozen. The `print_trainable_parameters()` should report a small fraction of total parameters as trainable (only the LoRA layers). For example, with r=64 on all linear layers, the trainable parameter count is on the order of a few hundred million (which is tiny relative to 70B).

## 3. Preparing the Dataset and Tokenization

Assume we have a Hugging Face `Dataset` of examples, each with a `"prompt"` (the fixed code + CVE description) and a `"target"` (the vulnerable code). We need to combine these into a single input sequence for the model, and mask the prompt part in the labels so that the loss is only computed on the target portion. 

We will concatenate the prompt and target with an end-of-sequence token, tokenize them, and set the labels such that tokens corresponding to the prompt are `-100` (ignored in loss calculation). This ensures a causal language modeling setup where the model is trained to predict only the vulnerable code given the prompt.

```python
from datasets import Dataset  # or DatasetDict if you have train/val

# Example: assume `raw_dataset` is your Dataset with 'prompt' and 'target' columns.
# If you have a DatasetDict with a train split, use raw_datasets["train"] accordingly.
raw_dataset = Dataset.from_dict({
    "prompt": ["<example prompt 1>", "<example prompt 2>"], 
    "target": ["<example vulnerable code 1>", "<example vulnerable code 2>"]
})
# Define a preprocessing function to tokenize and mask labels
def preprocess_example(example):
    prompt_text = example["prompt"]
    target_text = example["target"]
    # Concatenate prompt and target, with an EOS token at the end of target
    full_text = prompt_text + target_text + tokenizer.eos_token
    tokens = tokenizer(full_text, truncation=True)  # you can add max_length if needed
    input_ids = tokens["input_ids"]
    # Create labels copying input_ids, then mask prompt portion
    labels = input_ids.copy()
    prompt_len = len(tokenizer(prompt_text, add_special_tokens=False)["input_ids"])
    # Mask all prompt tokens (and padding if any) in the labels
    for i in range(prompt_len):
        labels[i] = -100
    # (Optional) also mask padding tokens just in case
    # But here we didn't add padding yet; will handle padding in collator.
    return {"input_ids": input_ids, "attention_mask": tokens["attention_mask"], "labels": labels}

# Apply preprocessing to the entire dataset
tokenized_dataset = raw_dataset.map(preprocess_example, remove_columns=raw_dataset.column_names)
```

**Data formatting tips:** Make sure each `prompt` already contains any necessary formatting that the model expects (for example, special tokens or separators between the fixed code and CVE description, if needed for clarity). In this simple example, we directly concatenate them. The model's EOS token is appended to mark the end of the response, which helps the model learn when to stop. By setting prompt tokens' labels to `-100`, we ensure the loss only comes from predicting the `target` (vulnerable code), not from regurgitating the prompt.

## 4. Setting Up Training Arguments

We use the Hugging Face `Trainer` API for convenience. Here are some important training hyperparameters and configurations for 2Ã—A100 GPUs:

- **Batch Size & Gradient Accumulation:** We set `per_device_train_batch_size=1` (one sequence per GPU at a time, given the model size) and use `gradient_accumulation_steps` to accumulate gradients over multiple steps for a larger effective batch. For example, with 8 accumulation steps, the effective batch size is 8 Ã— 1 = 8. Adjust this based on memory and dataset size.
- **Precision:** We enable `fp16=True` (or `bf16=True` if supported) to use half-precision gradients, which is standard when training with BF16 compute on A100.
- **Optimizer:** We use the 8-bit AdamW optimizer provided by bitsandbytes (`optim="paged_adamw_8bit"`), which is memory-efficient and was used in the QLoRA paper ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)).
- **Learning Rate & Scheduling:** A constant learning rate (`lr_scheduler_type="constant"`) with no warmup is often used for LoRA fine-tuning ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)). We choose a learning rate of 1e-4 for a 70B model (larger models often use slightly lower LR, e.g., QLoRA used 1e-4 for 33B/65B ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=33B%20All%2032%201e,4%202343%20512%201024)) and 2e-4 for 7B/13B).
- **Logging & Saving:** Configure `logging_steps` for frequent logging (since dataset is small) and `save_steps` or `save_strategy` to save checkpoints. We also set `max_grad_norm=0.3` for gradient clipping (per QLoRA's recommendations ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))).

```python
from transformers import TrainingArguments, Trainer

training_args = TrainingArguments(
    output_dir="./qlora-70b-cve",       # output directory for checkpoints
    overwrite_output_dir=True,
    num_train_epochs=3,                # small number of epochs for demonstration
    per_device_train_batch_size=1,
    gradient_accumulation_steps=8,     # accumulate gradients to simulate batch size 8
    learning_rate=1e-4,
    lr_scheduler_type="constant",      # constant LR (no decay)
    warmup_steps=0,
    optim="paged_adamw_8bit",          # 8-bit AdamW optimizer for efficiency ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer))
    fp16=True,                         # use FP16 mixed precision (or bf16=True if on A100 and desired)
    max_grad_norm=0.3,                 # gradient clipping for stability ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer))
    logging_steps=10,
    save_strategy="epoch",
    report_to="none",                  # (or "tensorboard"/"wandb" if logging to a tool)
    gradient_checkpointing=True        # Enable grad checkpointing to reduce memory usage
)
```

> **Note:** We enabled `gradient_checkpointing=True` to trade compute for lower memory usage, which is helpful for large models. With our model already sharded in 4-bit, this further ensures we stay within memory limits. Also, since we are using a single process with `device_map` for multi-GPU, we set `ddp_find_unused_parameters=False` by default (Trainer handles this) because only LoRA params are trainable and all should be used.

## 5. Initializing Trainer and Training on 2 GPUs

Now we create a `Trainer` with our model, dataset, and training arguments. We also need to handle padding in the data collator. The dataset examples may have different lengths, so we'll pad them to the same length per batch. We can define a custom `data_collator` that pads `input_ids`, `attention_mask`, and `labels` to the max length in the batch, using `tokenizer.pad_token_id` for padding and `-100` for label padding (so padded labels don't contribute to loss).

```python
# Define a custom data collator for padding
import numpy as np

def data_collator(batch):
    # Batch is a list of dicts with keys: input_ids, attention_mask, labels
    max_length = max(len(sample["input_ids"]) for sample in batch)
    # Pad sequences to max_length
    input_ids = []
    attention_masks = []
    labels = []
    for sample in batch:
        seq_len = len(sample["input_ids"])
        pad_len = max_length - seq_len
        # Pad input_ids and attention_mask
        input_ids.append(sample["input_ids"] + [tokenizer.pad_token_id] * pad_len)
        attention_masks.append(sample["attention_mask"] + [0] * pad_len)
        # Pad labels with -100
        labels.append(sample["labels"] + [-100] * pad_len)
    # Convert to tensors
    input_ids = torch.tensor(input_ids, dtype=torch.long)
    attention_masks = torch.tensor(attention_masks, dtype=torch.long)
    labels = torch.tensor(labels, dtype=torch.long)
    return {"input_ids": input_ids, "attention_mask": attention_masks, "labels": labels}

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_dataset,  # use tokenized_dataset (or tokenized_dataset["train"] if a DatasetDict)
    data_collator=data_collator
)
```

Everything is set up. Finally, we launch the training:

```python
trainer.train()
```

During training, you'll see logs every 10 steps (as set by `logging_steps`). The model weights remain in 4-bit on each GPU, and only the LoRA adapter weights are updated. After training, you can save the LoRA adapters (using `model.save_pretrained`) or the full model (which will still consist of the original 70B base in 4-bit plus the small LoRA weights).

**Multi-GPU usage:** Because we used `device_map="auto"` to shard the model, we run this script as a single process (no explicit `accelerate.launch` needed). The Trainer will utilize both GPUs as the model is split across them. If you instead wanted to use distributed data parallel (DDP) with model replication (not usually feasible for 70B on a single GPU), you would omit `device_map` and launch the script with `torchrun` or `accelerate`. In our case, sharding is necessary to fit the model, so one process per node is ideal.

## Summary

We have provided a comprehensive example of fine-tuning a 70B LLaMA 3 model with QLoRA on 2 A100 GPUs. The key steps include loading the model in 4-bit precision ([Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA](https://huggingface.co/blog/4bit-transformers-bitsandbytes#:~:text=from%20transformers%20import%20BitsAndBytesConfig)), applying LoRA to all linear layers (r=64, Î±=16) ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=All%20models%20use%20NormalFloat4%20datatype,schedule%20and%20paged%20AdamW%20optimizer)), preparing prompt-target sequences with proper label masking, and leveraging the Hugging Face Trainer with 8-bit optimizers ([timdettmers/qlora-unnatural-instructions-65b Â· Hugging Face](https://huggingface.co/timdettmers/qlora-unnatural-instructions-65b#:~:text=linear%20layers%20with%20BFloat16%20as,schedule%20and%20paged%20AdamW%20optimizer)) and gradient accumulation for efficient multi-GPU training. This script should be ready to run, assuming the dataset and model are correctly specified. Happy fine-tuning!









xxxxx



Got it. Iâ€™ll gather the best practices for preparing the cveFixes1 datasetâ€”specifically targeting C codeâ€”for fine-tuning a LLaMA 3 model using PEFT/QLoRA. The goal is to create a text-to-text format where the model learns to insert a specific CVE into safe code, transforming it into a vulnerable version.

Iâ€™ll also clarify the exact semantics of `function_before` and `function_after` in this dataset to confirm which one is vulnerable and which is fixed.

Iâ€™ll be back shortly with the complete guidance.

# Fine-Tuning LLaMA&nbsp;3 on CVE Vulnerability Injection (QLoRA Guide)

## Understanding the CVE Fixes Dataset (`cveFixes1`)

The **cveFixes1** dataset is a collection of code changes associated with CVE fixes across multiple programming languages (C, Go, Java, Python, Ruby). Each entry contains a code snippet **before** and **after** a vulnerability fix, along with metadata like CVE and CWE IDs. In particular, for each C function in the dataset: 

- **`func_before`**: The function **before** the vulnerability was fixed â€“ this is the vulnerable version of the code. In other words, if an entry is labeled as vulnerable, `func_before` contains the code with the CVE vulnerability present.  
- **`func_after`**: The function **after** the vulnerability was fixed â€“ this is the patched (non-vulnerable) version of the code.

For our task, we want the model to learn to **insert a specific CVE vulnerability** into a given piece of safe code. This means our training will use the **fixed code** plus its CVE ID as input, and the **vulnerable code** as the target output. Concretely:
- **Input**: Non-vulnerable function (patched code from `func_after`) + an indicator of the vulnerability (the `cve_id`).  
- **Output**: Vulnerable version of that function (code from `func_before`, which includes the vulnerability).

The datasetâ€™s C subset has about 2,621 examples of such C function pairs. Each example is one function where a CVE-related fix was applied. (Note: Some functions can be quite large, so we may need to handle or filter lengthy examples that exceed the modelâ€™s context window.)

## Preparing the Dataset for Text-to-Text Fine-Tuning

To fine-tune LLaMAÂ 3 in a text-to-text fashion, we need to format each example into a prompt and an expected completion. We will create a **prompt** that includes the fixed code and the CVE ID, and a **target** that is the vulnerable code. Hereâ€™s a step-by-step plan:

1. **Load the Dataset**: Use Hugging Faceâ€™s `datasets` library to load the C subset of cveFixes1. For example:  
   ```python
   from datasets import load_dataset
   data = load_dataset("euisuh15/cveFixes1", split="c")
   ```  
   This gives a dataset where each item has fields like `func_before`, `func_after`, and `cve_id`.

2. **Define the Prompt-Output Format**: We need a clear format so the model can distinguish input from output during training. A simple approach is to concatenate the CVE ID and the fixed code with some separators or prompt text. For example, one might format the training pair as:  
   **Prompt:** *"CVE-{ID}: Below is a C function with the vulnerability fixed. Insert the vulnerability back into this function.\n<code_after_snippet>\nVulnerable version:"*  
   **Target:** *"<code_before_snippet>"* (the vulnerable code).

   In practice, you can make the prompt more straightforward since we have a consistent task. For instance:  
   ```text
   [CVE-2021-1234] 
   Non-vulnerable code: 
   <func_after code here> 
   Vulnerable code:
   ```  
   The model would then be trained to produce the vulnerable code after the "Vulnerable code:" prompt.

3. **Construct Prompt-Target Pairs**: Using the loaded dataset, iterate through each example and build the prompt and target strings. In code, this might look like:  
   ```python
   def make_prompt_and_target(example):
       cve = example["cve_id"]
       fixed_code = example["func_after"]
       vuln_code = example["func_before"]
       prompt = f"CVE-{cve}:\nFixed function:\n{fixed_code}\n\nVulnerable version:\n"
       target = vuln_code
       return {"prompt": prompt, "target": target}
   
   data = data.map(make_prompt_and_target, remove_columns=data.column_names)
   ```  
   This will produce a new dataset with just `"prompt"` and `"target"` fields for each sample, where `"prompt"` is the input text and `"target"` is the expected output text.

4. **Verify Example Formatting**: Itâ€™s good to double-check one example to ensure the format is correct. For instance:  
   **Prompt example:**  
   ```
   CVE-2014-0160:
   Fixed function:
   static int heartbeat() {
       // ... (safe code)
   }
   
   Vulnerable version:
   ```  
   **Target example:** (the model should output)  
   ```
   static int heartbeat() {
       // ... (code with the Heartbleed vulnerability)
   }
   ```  
   Make sure the prompt clearly separates the fixed code from where the vulnerable code should start. The newline and "Vulnerable version:" (or any chosen marker) before the target help indicate where the modelâ€™s output begins.

## Choosing a Data Format (JSONL vs. HuggingFace Dataset)

For fine-tuning with Hugging Face Transformers and PEFT, using a **Hugging Face Dataset** object is very convenient. The `datasets` library can handle large datasets efficiently and works well with the Trainer API. Two common approaches are:

- **Hugging Face `Dataset` Object**: After creating the prompt-target pairs as shown above, you can keep the data in a `Dataset` and feed it to the training pipeline. This is convenient for directly using the Hugging Face Trainer, which can take a `Dataset` for training. You may further split into train/val if needed (for example, `data.train_test_split`).

- **JSONL (JSON Lines) File**: This is a simple text file format where each line is a JSON object. You could export the dataset to JSONL if you plan to use custom loading or need to share the processed data. For example, Hugging Face Datasets allows saving to disk:  
  ```python
  data.to_json("finetune_cve_injection.jsonl", orient="records", lines=True)
  ```  
  This would produce a file where each line has `{"prompt": "...","target": "..."}`. JSONL is human-inspectable and easily loaded if you prefer writing a custom DataLoader.

In practice, if you intend to use the Hugging Face Trainer/PEFT pipeline, you can keep the data in memory as a `Dataset`. If you prefer a custom PyTorch training loop, you might convert it to a PyTorch `DataLoader`. Either approach is fine; what matters is that the format cleanly separates input and output text. 

**Recommendation**: Use the Hugging Face Dataset format for integration with Transformers. It natively supports methods to tokenize and collate data. You can always export to JSONL for backup or debugging, but itâ€™s not strictly required for fine-tuning.

## Preprocessing and Tokenization for LLaMAÂ 3

Before training, we need to tokenize our prompt and target texts using LLaMAÂ 3â€™s tokenizer. LLaMAÂ 3 (like LLaMA 2) is a decoder-only model, meaning it expects a single sequence of tokens as input, and we will train it in a causal language modeling manner. Key considerations:

- **Load the Tokenizer**: Use the appropriate tokenizer for the LLaMAÂ 3 model (from Hugging Face, e.g., `AutoTokenizer`). For example:  
  ```python
  from transformers import AutoTokenizer
  tokenizer = AutoTokenizer.from_pretrained("meta-llama/Llama-3-7b-hf")  # hypothetical name
  tokenizer.pad_token = tokenizer.eos_token  # LLaMA uses EOS as padding if needed
  ```  
  Ensure you have the correct model checkpoint name for LLaMAÂ 3.

- **Tokenize Prompt and Target**: We will combine the prompt and target into one sequence for the model, but we need to keep track of which tokens are the prompt (so we can mask them out in the loss). A common strategy is:  
  1. Tokenize the **prompt** and the **target** separately.  
  2. Concatenate them, and create a label array that is `-100` (ignore index) for all prompt tokens and equals the token IDs for the target part ([Large Language Model Finetuning Practice | by Haifeng Zhao](https://medium.com/@piscaries/large-language-model-finetuning-practice-7e131291046e#:~:text=,return)). This way, during training the loss is only computed for the target (vulnerable code) tokens, not for regurgitating the prompt. 

  For example:  
  ```python
  def tokenize_func(example):
      prompt_ids = tokenizer.encode(example["prompt"], add_special_tokens=False)
      target_ids = tokenizer.encode(example["target"], add_special_tokens=False)
      # Ensure an EOS token between or at end:
      input_ids = prompt_ids + target_ids + [tokenizer.eos_token_id]
      # Create labels: -100 for prompt, and actual ids for target (plus EOS)
      labels = [-100] * len(prompt_ids) + target_ids + [tokenizer.eos_token_id]
      return {"input_ids": input_ids, "labels": labels}
  
  tokenized_data = data.map(tokenize_func, remove_columns=["prompt","target"])
  ```  
  In this scheme, `input_ids` contains the whole sequence (prompt followed by target), and `labels` has `-100` for each prompt token position (so the model isnâ€™t trained to output the prompt) and the actual token IDs for the vulnerable code portion ([Large Language Model Finetuning Practice | by Haifeng Zhao](https://medium.com/@piscaries/large-language-model-finetuning-practice-7e131291046e#:~:text=,return)).

- **Handle Length and Truncation**: LLaMAÂ 3 models typically have a context length (e.g., 2048 or 4096 tokens). Some functions in cveFixes1 are very large, possibly exceeding this. You should decide on a maximum sequence length (perhaps the modelâ€™s max context or a bit less to allow for the prompt + output). In the tokenization step, use `tokenizer(truncation=True, max_length=N)` for a safe `N` (like 1024 or 2048, depending on GPU memory and model context). Truncate from the end of the sequence if needed (or from the code â€“ but generally you want to ensure the beginning of the prompt is intact, which it will be if truncating at end, possibly cutting off some of the target if itâ€™s too long).

- **Padding**: If you use a `DataCollator` to batch examples, ensure it handles padding. For example, `DataCollatorForLanguageModeling` with `mlm=False` will pad sequences to the same length in a batch and set padding token labels to `-100` by default. Since we already set `-100` for prompt tokens, any additional padding should also be `-100` (the collator typically does this). Setting `tokenizer.pad_token = tokenizer.eos_token` (as above) helps avoid errors with models that have no pad token.

- **Validation**: Itâ€™s wise to double-check that for a sample, the `input_ids` correspond to `[Prompt tokens] [Target tokens] <eos>` and `labels` are `[-100,...-100, token_ids_of_target..., eos_id]`. This ensures the model will learn to predict only the vulnerable code given the fixed code context.

## Setting Up LLaMAÂ 3 with QLoRA (via PEFT)

With our data ready, we can now configure the model for efficient fine-tuning. Weâ€™ll use **QLoRA**, which means we load LLaMAÂ 3 in 4-bit precision and attach LoRA adapters (low-rank weight updates) to train on our dataset. The Hugging Face [PEFT](https://github.com/huggingface/peft) library makes this straightforward. Here are the steps:

1. **Load the Base Model in 4-bit**: Use `bitsandbytes` integration to load the model in 4-bit precision. We create a `BitsAndBytesConfig` for 4-bit NormalFloat (NF4) quantization. For example:  
   ```python
   from transformers import AutoModelForCausalLM, BitsAndBytesConfig
   bnb_config = BitsAndBytesConfig(
       load_in_4bit=True,
       bnb_4bit_quant_type="nf4",
       bnb_4bit_use_double_quant=True,
       bnb_4bit_compute_dtype=torch.float16  # or torch.bfloat16 if using newer GPUs
   )
   model = AutoModelForCausalLM.from_pretrained(
       "meta-llama/Llama-3-7b-hf", 
       quantization_config=bnb_config,
       device_map="auto"  # if using multiple GPUs or "cpu"/"cuda" as needed
   )
   ```  
   This loads the LLaMAÂ 3 model weights in 4-bit precision. The `nf4` quantization type and double quantization are recommended for QLoRA. 

2. **Prepare Model for QLoRA Training**: Thereâ€™s a utility in PEFT to prepare a quantized model for training. This will tweak some model settings (like turning off weight decay for certain layers, etc.). Use:  
   ```python
   from peft import prepare_model_for_kbit_training
   model = prepare_model_for_kbit_training(model)
   ```  
   This step handles some low-level fixes (for example, making sure gradient checkpointing is enabled and some layers are cast to FP32 as needed). Now the model is ready for attaching LoRA adapters.

3. **Configure LoRA**: Decide on LoRA hyperparameters. Key parameters are:
   - `r`: LoRA rank (the dimensionality of the update matrices). Higher `r` means more capacity to learn (but also more memory use). Common values are 8, 16, 32, or 64. For an 8â€“13B model, `r=16` or `32` is often used; for very complex tasks or larger models, `r=64` can be beneficial at the cost of more VRAM. You can start with `r=16` and increase if the model underfits.
   - `lora_alpha`: Scaling factor for the LoRA updates. Often set proportional to `r` (like alpha = r or 2*r). For example, many LoRA setups use `alpha = 2*r` or `alpha = r`. In one LLaMA-3 8B example, `r=64` and `alpha=16` was used (so alpha was smaller in that case). The QLoRA paper often used Î±=16 or 32 for various setups. You can set `lora_alpha=16` as a reasonable default and adjust if needed.
   - `lora_dropout`: Dropout applied to LoRA layers. If your dataset is small (a few thousand examples), a bit of dropout (e.g. 0.05â€“0.1) helps prevent overfitting. If you have plenty of data or want maximum retention of details, you can use 0.0.
   - `target_modules`: Which layers to apply LoRA to. For LLaMA models, the safe bet is to target all the key weight matrices in the Transformer blocks (e.g. the query, key, value, and output projection of self-attention). In LLaMA these are named `"q_proj"`, `"k_proj"`, `"v_proj"`, `"o_proj"`. Some setups also target the feed-forward layers (e.g., `"gate_proj"`, `"down_proj"`, `"up_proj"` for LLaMA-2 architecture) by using `target_modules="all-linear"` which applies LoRA to *every* linear layer. Targeting all linear layers can improve fine-tuning efficacy at the cost of more parameters. For initial experiments, you can stick with attention projections only, and later consider expanding if needed.
   - `bias`: Usually `"none"` (we donâ€™t train any bias terms in LoRA).

   Now create the LoRA config and wrap the model:  
   ```python
   from peft import LoraConfig, get_peft_model
   lora_config = LoraConfig(
       r=16,
       lora_alpha=16,
       target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],  # or "all-linear"
       lora_dropout=0.05,
       bias="none",
       task_type="CAUSAL_LM"
   )
   model = get_peft_model(model, lora_config)
   model.print_trainable_parameters()
   ```  
   This will transform the model so that it has trainable LoRA layers (and all original weights are frozen in 4-bit). The `print_trainable_parameters()` should report a small fraction of total parameters as trainable. For example, with `r=16` on a 7B model, trainable params might be on the order of tens of millions (<<1% of total).

## Training Configuration and Execution (QLoRA Fine-Tuning)

With model and data ready, the final step is to set up training. Key aspects include the training loop or Trainer, learning rate, batching, and number of epochs:

- **Training Loop**: You can use the Hugging Face `Trainer` API to simplify training. It will handle feeding data to the model and applying optimizers. For QLoRA, it's recommended to use a **paged AdamW optimizer** from bitsandbytes for efficiency ([QLoRA Â· GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,enable%20for)). Fortunately, Transformers v4.30+ allows specifying `optim="paged_adamw_8bit"` in `TrainingArguments` to use the 8-bit optimizer which is memory-efficient for LoRA ([QLoRA Â· GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,enable%20for)). 

  Set up `TrainingArguments` with appropriate values:  
  ```python
  from transformers import TrainingArguments, Trainer, DataCollatorForLanguageModeling

  training_args = TrainingArguments(
      output_dir="./llama3-cve-qlora",
      per_device_train_batch_size=1,  # adjust based on GPU memory
      gradient_accumulation_steps=8,  # accumulate grads for larger effective batch
      num_train_epochs=3,
      learning_rate=2e-4,
      warmup_steps=100,
      logging_steps=50,
      optim="paged_adamw_8bit",
      fp16=True,  # use mixed precision if supported
      evaluation_strategy="no",  # or "steps"/"epoch" if you have a eval set
      save_strategy="epoch",
      report_to="none"
  )

  # Data collator to pad sequences and mask out prompt portion appropriately
  data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)
  trainer = Trainer(
      model=model,
      args=training_args,
      train_dataset=tokenized_data,
      data_collator=data_collator
  )
  trainer.train()
  ```  
  In the above: 
  - `per_device_train_batch_size` is set to 1 as a safe default given the potentially long sequences; you can increase it if memory allows.
  - `gradient_accumulation_steps=8` effectively means a batch of 8 sequences before an optimizer step (simulate batch_size = 8). Adjust this along with the per-device batch to achieve a desired effective batch size.
  - `learning_rate` for LoRA is often in the range 2e-4 to 3e-4 ([QLoRA Â· GitHub](https://gist.github.com/SauravMaheshkar/4134e86f0d637d03699f098a8ea3d7e8?short_path=77b237e#:~:text=,%29%2C%5Cn)). We use 2e-4 with a small warmup. Monitor loss and adjust if needed (if underfitting, could increase; if unstable, decrease).
  - `fp16=True` to use half-precision compute for speed (since the base model is 4-bit, this mainly affects optimizer states and LoRA computations).
  - We turned off actual evaluation (`evaluation_strategy="no"`) for simplicity, but if you have a validation split, you can set it to evaluate every X steps or each epoch.
  - We use the `DataCollatorForLanguageModeling` with `mlm=False`, which will ensure that the `labels` tensor is correctly created. Since we already set up `labels` in the dataset with `-100` for prompt tokens, the collator should leave those as is. It will pad `input_ids` and `labels` to the longest sequence in each batch, padding labels with `-100` as needed (so padding tokens are ignored in loss).

- **Training Process**: As training runs, the model will learn to generate the vulnerable code given the fixed code and CVE context. Watch the training loss to ensure it's decreasing. Given the dataset size (~2.6k examples for C), a few epochs (2â€“4) are usually enough. Too many epochs might cause overfitting (the model might memorize patterns and not generalize).

- **PEFT Saving**: By default, Trainer will save the full model (with LoRA adapters merged into a PeftModel). You can also choose to save just the LoRA adapter weights (using `model.save_pretrained` from PEFT). If using the Trainerâ€™s default save, it will save a minimal checkpoint that includes the base model reference and the LoRA weights â€“ which is fine. After training, you can reload the model for inference via:  
  ```python
  from peft import PeftModel, PeftConfig
  base_model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3-7b-hf", device_map="auto", quantization_config=bnb_config)
  model = PeftModel.from_pretrained(base_model, "./llama3-cve-qlora")
  model.eval()
  ```
  This loads the base 4-bit model and then applies the trained LoRA weights.

- **Inference Check**: To test, feed a prompt in the same format as training (fixed code + CVE) and see if the model generates a plausible vulnerable version. Make sure to stop generation at an appropriate token (you might rely 










